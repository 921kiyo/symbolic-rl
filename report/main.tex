\documentclass[11pt,twoside]{report}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage[table]{xcolor}
\usepackage[toc,page]{appendix}
\usepackage{amssymb}
\usepackage{csquotes}
\usepackage{amsthm}
\usepackage{subdepth}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{color}
\usepackage{xcolor}
% \usepackage{minted}

\definecolor{codegray}{rgb}{0.8,0.8,0.8}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\input{prolog}

\definecolor{commentsColor}{rgb}{0.497495, 0.497587, 0.497464}
\definecolor{keywordsColor}{rgb}{0.000000, 0.000000, 0.635294}
\definecolor{stringColor}{rgb}{0.558215, 0.000000, 0.135316}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
%   basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=t,                    % sets the caption-position to bottom
  commentstyle=\color{commentsColor}\textit,    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=tb,	                   	   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{keywordsColor}\bfseries,       % keyword style
  language={},                 % the language of the code (can be overrided per snippet)
  %   language=Prolog,                 % the language of the code (can be overrided per snippet)
  otherkeywords={*,...},           % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{commentsColor}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{stringColor}, % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname,                  % show the filename of files included with \lstinputlisting; also try caption instead of title
  columns=fixed                    % Using fixed column width (for e.g. nice alignment)
}


%\usepackage[utf8]{inputenc}
%\usepackage[acronym]{glossaries} 
%\makeglossaries
%
%\newacronym{gcd}{GCD}{Greatest Common Divisor}
% 
%\newacronym{lcm}{LCM}{Least Common Multiple}
%\usepackage{nomencl}
%\makenomenclature
%\renewcommand{\nomname}{Time Zones}

%\usepackage[acronym,nomain,nonumberlist]{glossaries}
%\makeglossaries
%\newacronym{ny}{NY}{New York}
%\newacronym{la}{LA}{Los Angeles}
%\newacronym{un}{UN}{United Nations}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[chapter] % Reset theorem numbering  for each chapter

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition} % Definition numbers are dependent on theorem numbers
\newtheorem{exmp}[thm]{Example} % same for example numbers

% This is for chapter outline
\newtheorem{innercustomthm}{Chapter}
\newenvironment{customthm}[1]
 {\renewcommand\theinnercustomthm{#1}\innercustomthm}
 {\endinnercustomthm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Definitions for the title page
% Edit these to provide the correct information
% e.g. \newcommand{\reportauthor}{Timothy Kimber}
\DeclareMathOperator{\E}{\mathbb{E}}

\newcommand{\reporttitle}{Symbolic Reinforcement Learning using Inductive Logic Programming}
\newcommand{\reportauthor}{Kiyohito Kunii}
\newcommand{\supervisor}{Prof. Alessandra Russo \\ Mark Law \\  Ruben Vereecken}
\newcommand{\degreetype}{MSc in Computing Science}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% load some definitions and default packages
\input{includes}

% load some macros
\input{notation}

\date{September 2018}

\newtheorem{examp}{Example}[section]

% Combine list of figures and tables
\makeatletter
\renewcommand*{\ext@figure}{lot}
\let\c@figure\c@table
\let\ftype@figure\ftype@table
\let\listoftableandfigures\listoftables
\renewcommand*\listtablename{List of Tables and Figures}
\makeatother

\begin{document}

% load title page
\input{titlepage}

% page numbering etc.
\pagenumbering{roman}
\clearpage{\pagestyle{empty}\cleardoublepage}
\setcounter{page}{1}
\pagestyle{fancy}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{abstract}
\input{chapters/abstract}
 \end{abstract}
\cleardoublepage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgments}

I would like to thank Prof. Alessandra Russo for supervising my project, and for her enthusiasm for my work and invaluable guidance throughout.

I would also like to thank Mark Law for his expertise on inductive logic programming and answer set programming,  as well as many fruitful discussions, and  Ruben Vereecken for his expertise on reinforcement learning and for providing me with advice and assistance for technical implementation.

\clearpage{\pagestyle{empty}\cleardoublepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%--- table of contents
\fancyhead[RE,LO]{\sffamily {Table of Contents}}
\tableofcontents

% Add blank page
\clearpage{\pagestyle{empty}\cleardoublepage}
\pagenumbering{arabic}
\setcounter{page}{1}
\fancyhead[LE,RO]{\slshape \rightmark}
\fancyhead[LO,RE]{\slshape \leftmark}

% List of Figures and Tables
\renewcommand\listtablename{List of Figures and Tables}
\listoftableandfigures

%\clearpage{\pagestyle{empty}\clearpage}
%\glsaddall
%\printglossary[type=\acronymtype,title=Acronyms]
%\printglossary[type=\acronymtype]
 
%\printglossary

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% IMPERIAL LOGO
% \begin{figure}[tb]
% \centering
% \includegraphics[width = 0.4\hsize]{./figures/imperial}
% \caption{Imperial College Logo. It's nice blue, and the font is quite stylish. But you can choose a different one if you don't like it.}
% \label{fig:logo}
% \end{figure}
% Figure~\ref{fig:logo} is an example of a figure.

\chapter{Introduction}
\label{introduction}
\input{chapters/introduction}

\chapter{Background}
\label{background}

This chapter introduces the necessary background on Answer Set Programming (ASP), Inductive Logic Programming (ILP) and Reinforcement Learning (RL), which provide the foundations of our research.

\section{Answer Set Programming (ASP)}
\label{sec:asp}
Answer Set Programming (ASP) is a form of declarative programming aimed at knowledge-intensive applications such as difficult search problems \cite{Lifschitz2008}. 
We first introduce a stable model which is the foundation of ASP, and describe ASP syntax. 

% Non-monotonic reasoning
\subsection{Stable Model Semantics}

The semantics of the logic are based on the notion of interpretation, which is defined under a \textit{domain}. A domain contains all the objects that exist in a given problem. 
In logic, it is convention to use a special interpretation called a \textit{Herbrand interpretation}.

\begin{defn}
\label{def:definite_logic_program}
\textit{Definite Logic Program} is a set of definite rules, and  
A \textit{definite rule} is of the form: 
\begin{equation}
\label{definite_rule}
\begin{split}
\underbrace{h}_\text{head} \leftarrow\underbrace{ a\textsubscript{1}, a\textsubscript{2}, ..., a\textsubscript{n}}_\text{body}
\end{split}
\end{equation}
\textit{h} and  \textit{a}\textsubscript{1}, ..., \textit{a}\textsubscript{n} are all atoms. \textit{h} is the \textit{head} of the rule and \textit{a}\textsubscript{1}, ..., \textit{a}\textsubscript{n} are the \textit{body} of the rule.
\end{defn}

\begin{defn}
\label{def:normal_logic_program}
\textit{Normal Logic Program} is a set of normal rules,
A \textit{normal rule} is: 
\begin{equation}
\label{normal_rule}
\begin{split}
\underbrace{h}_\text{head} \leftarrow\underbrace{ a\textsubscript{1}, a\textsubscript{2}, ..., a\textsubscript{n}, not \ b\textsubscript{n+1}, ....not \ b\textsubscript{n+k}}_\text{body}
\end{split}
\end{equation}
where h, a\textsubscript{1}, ... a\textsubscript{n}, and b\textsubscript{n+1}, ..., b\textsubscript{n+k} are all atoms.
\end{defn}

\begin{defn}
\label{def:herbrand_domain}
The \textit{Herbrand Domain}, or \textit{Herbrand Universe}, of a normal logic program \textit{P}, denoted \textit{HD(P)}, is the set of all ground terms constructed fromconstants and function symbols that appear in \textit{P}.
\end{defn}

\begin{defn}
\label{def:herbrand_base}
The \textit{Herbrand Base} of a normal logic program \textit{P}, denoted \textit{HB(P)}, is the set of all ground atoms that are formed by predicate symbols in \textit{P} and terms in \textit{HD(P)}.
\end{defn}

\begin{defn}
The \textit{Herbrand Interpretation} of a program \textit{P}, denoted \textit{HI(P)}, is a subset of the HB(P), and ground atoms in an HI(R) are assumed to be true.
\end{defn}

In order to define a stable model, we need to define a minimal Herbrand model of a normal logic program \textit{P}.

\begin{defn}
\label{def:herbrand_model}
The \textit{Herbrand Model} of a definite logic program \textit{P} is a \textit{HI(P)} that satisfies all the clauses in \textit{P}. In other words, the set of clauses \textit{P} is unsatisfiable if no Herbrand model is found.
\end{defn}

\begin{defn}
\label{def:minimal_herbrand_model}
The \textit{Minimal Herbrand Model} of a normal logic program \textit{P} is a Herbrand model \textit{M} if none of the subset of \textit{M} is a Herbrand model of \textit{P}.
\end{defn}

% \begin{defn}
% \textit{Least Herbrand Model} (denoted as \textit{M(P)}) is an unique minimal Herbrand model for definite logic programs.  An Herbrand Model is a minimum Herbrand model if and only if none of its subsets is an Herbrand model.
% \end{defn}
% For normal logic programs, there may not be a least Herbrand Model.

%Interpretation evaluate it to true
%Interpretation evaluate it to false

\begin{examp} \normalfont (Herbrand Interpretation, Herbrand Model and M(P)) \\
We use an simple example to highlight the definitions of Herbrand Interpretation and Herbrand Model.\\

P = $\begin{cases}
	\textit{drive(X)}  \leftarrow \textit{hasCar(X).} \\
	\textit{hasCar(john).}
      \end{cases}$ HD(P) = \{ john \} , HB(P) = \{ hasCar(john), drive(john) \}  \\

Given the above program, there are four Herbrand Interpretations:\\
HI(P) = $\langle$ \{hasCar(john)\}, \{drive(john)\}, \{hasCar(john), drive(john)\}, \{\} $\rangle$, \\
and one Herbrand Model (as well as M(P)) = \{q(a), p(a)\}.

\end{examp}
To solve a normal logic program \textit{P}, the program \textit{P} needs to be grounded. The \textit{grounding} of \textit{P} is the set of all clauses  c $\in$ \textit{P} and with variables replaced by all possible terms in the Herbrand Domain.
\begin{defn}
The algorithm of grounding starts with an empty program \textit{Q} = \{  \} and the relevant grounding is constructed by adding each rule \textit{R} to \textit{Q} such that:
\begin{itemize}
\item \textit{R} is a ground instance of a rule in \textit{P}.
\item Their positive body literals already occurs in the in the rules in \textit{Q}.
\end{itemize}
The algorithm terminates when no more rules can be added to \textit{Q}.
\end{defn}

\begin{examp} \normalfont (Grounding) \\
\label{grounding}

P = $\begin{cases}
	\textit{drive(X)}  \leftarrow \textit{hasCar(X).} \\
	\textit{hasCar(john).}
      \end{cases}$ \\

ground(P) in this example is \{hasCar(john), drive(john)\}, as hasCar(john) is already grounded and added to Q, and it is also a positive body literal for the first rule, resulting in q(a).
\end{examp}

%TODO Explain grounding in ASP context.
%The grounding of a normal logic program P can be obtained by replacing each rule in P with a ground instance of the rule, such that for each atom A in body\textsuperscript{+} (R) (TODO EXPLAIN WHAT THIS IS), already occurs in the head of another ground rule.
The entire program must not only be grounded in order for an ASP solver to work, but each rule must also be \textit{safe}. 
A rule \textit{R} is safe if every variable that occurs in the head of the rule occurs at least once in body\textsuperscript{+}(R).
Since there is no unique least Herbrand Model for a normal logic program, Stable Model of a normal logic program is defined in \cite{Gelfond1988}. 
In order to obtain the Stable Model of a program P, P needs to be converted using \textit{Reduct} with respect to an interpretation X.

\begin{defn}
The \textit{reduct} of P with respect to any set X of ground atoms of a normal logic program can be constructed such that:
\begin{itemize}
\item If the body of any rule in P contains an atom which is not in X, those rules need to be removed.
\item All default negation atoms in the remaining rules in P need to be removed.
\end{itemize}
\end{defn}

%Any stable model is a minimal Herbrand model, and stable sets is stable models. The stable models can be found by constructing the result of the program with respect to sets of atoms X (P\textsuperscript{x} in the following 2 steps
\begin{defn}
X is a \textit{stable model} of P if it is the least Herbrand Model of P\textsuperscript{X}.
\end{defn}

\begin{examp} \normalfont (Reduct) \\

P = $\begin{cases}
	\textit{male(X)}  \leftarrow \textit{not female(X).} \\
  	\textit{female(X)} \leftarrow \textit{not male(X).} \\
      \end{cases}$  X = \{male(john), female(anna)\}

Where X is a set of atoms. ground(P) is

male(john)  $\leftarrow$ not\ female(john). \\
male(anna)  $\leftarrow$ not\ female(anna). \\
female(john) $\leftarrow$ not\ male(john). \\
female(anna) $\leftarrow$ not\ male(anna). \\

The first step removes male(anna)  $\leftarrow$ not\ female(anna). and female(john) $\leftarrow$ not\ male(john).\\

male(john)  $\leftarrow$ not\ female(john). \\
female(anna) $\leftarrow$ not\ male(anna). \\

The second step removes negation atoms from the body. \\
Thus reduct P\textsuperscript{x} is (ground(P))\textsuperscript{x} =  \{male(john), female(anna).\}
\label{reduct}
\end{examp}

\begin{defn}
X is a \textit{stable model} of P if it is the least Herbrand Model of P\textsuperscript{X}.
\end{defn}

For normal logic program, there may be more than one or no stable models. In stable model semantics,  there are two different notion of entailments.
\begin{defn}
An atom $a$ is \textit{bravely} entailed by a normal logic program P, denoted $ P \models\textsubscript{b} \ a$, if it is true in at least one stable model of P. An atom $a$ is \textit{cautiously} entailed by a normal logic program P, denoted $ P \models\textsubscript{c} \ a$ if it is true in every stable model of P \ref{Law2014}.
\end{defn}

\begin{examp} \normalfont (Brave and Cautious Entailment) \\

P = $\begin{cases}
	\textit{male}  \leftarrow \textit{not female, tall.} \\
  	\textit{female} \leftarrow \textit{not male, tall.} \\
	\textit{tall.}
      \end{cases}$ \\

In this case, $P\models$\textsubscript{b} \{male, female, tall\} and $P\models$\textsubscript{c} \{tall\}
\end{examp}

\subsection{Answer Set Programming (ASP) Syntax}

An answer set of normal logic program P is a stable model defined by a set of rules, where each rule consists of literals, which are made up with an atom \textit{p} or its default negation \textit{not p} (\textit{negation as failure}). Answer Set Programming (ASP) is a normal logic program with extensions: constraints, choice rules and optimisation statements.

%TODO positive rule and definite clause
A \textit{constraint} of the program P is of the form:
\begin{equation}
\begin{split}
\leftarrow \textit{a}\textsubscript{1}, ..., \textit{a}\textsubscript{n}, \textit{not b}\textsubscript{n}, ..., \textit{not b}\textsubscript{n+k}
\end{split}
\label{asp_choice_rule}
\end{equation}
where the normal rule has an empty head, and a normal rule  with empty body is a \textit{fact}.
 The constraint filters any irrelevant answer sets. When computing ground(P)\textsubscript{x}, the empty head becomes $\perp$, which cannot be in the answer sets. So any answer set that includes ground body of a constraint is eliminated.

A \textit{choice rule} can express possible outcomes given an action choice, which is of the form:
\begin{equation}
\begin{split}
\underbrace{l\{h\textsubscript{1},...,h\textsubscript{m}\}u}_\text{head} \leftarrow \underbrace{a\textsubscript{1}, ..., a\textsubscript{n}, not\ b\textsubscript{n+1}, ..., not\ b\textsubscript{n+k}}_\text{body}
\end{split}
\label{asp_choice_rule}
\end{equation}

where \textit{l} and \textit{u} are integers and h\textsubscript{i} for 1 $\leq$ i $\leq$ m are atoms. The head in the choice rule is called \textit{aggregates}.
Informally, if the body of a ground choice rule is satisfied by an interpretation X, given a ground choice rule, the choice rule generates all answer sets in which  $l \leq | X \cap \{h\textsubscript{1},...,h\textsubscript{m}\} | \leq u$
\ref{Law2014}.

\textit{Optimisation statement} is useful to sort the answer sets in terms of preference, which is of the form:
\begin{equation}
\begin{split}
&\textit{\#minimize[a\textsubscript{1}=w\textsubscript{1},...a\textsubscript{n}=w\textsubscript{n}]} \ \textit{or} \\
&\textit{\#maximize[a\textsubscript{1}=w\textsubscript{1},...a\textsubscript{n}=w\textsubscript{n}]} \\
\end{split}
\label{asp_optimisation_statement}
\end{equation}

where \textit{w\textsubscript{1},..., w\textsubscript{n}} are integer weights and \textit{a\textsubscript{1},...,a\textsubscript{n}} are ground atoms.  ASP solvers compute the scores of the weighted sum of the sets of ground atoms based on the true answer sets, and find optimal answer sets which either maximise or minimise the score.

\textit{Clingo} is one of the modern ASP solvers that executes the ASP program and returns answer sets of the program \cite{Gebser2011}. We will use Clingo 5 for the implementation of our new framework.

%An answer set of ASP program is interpretations that make all the ruls true.
%Non-monotonicity.
%TODO ASP has true, false and unknown

\section{Inductive Logic Programming (ILP)}
\label{sec:ilp}

\textit{Inductive Logic Programming (ILP)} is a subfield of machine learning aimed at supervised inductive concept learning, and is the intersection between machine learning and logic programming \cite{Muggleton1991}. 
The purpose of ILP is to inductively derive a hypothesis \textit{H} that is a solution of a learning task, which covers all of the positive examples and none of the negative examples, given a hypothesis language for search space and cover relation \cite{DeRaedt1997}. 
ILP is based on learning from entailment, as shown in Equation \ref{ilp_equation}.

\begin{equation}\label{ilp_equation}
B \wedge H \models E
\end{equation}

where \textit{E} contains all of the positive examples, denoted \textit{E\textsuperscript{+}}, and none of the negative examples, denoted \textit{E\textsuperscript{-}}.

An advantage of ILP over statistical machine learning is that the hypothesis that an agent learns can be easily understood by a human, as it is expressed in first-order logic, making the learning process explainable.
By contrast, a limitation of ILP is scalability. There are usually thousands or more examples in many real-world examples. Scaling ILP tasks to cope with large examples is a challenging task \cite{Muggleton1993}.

\subsection{ILP under Answer Set Semantics}
There are several ILP non-monotonic learning frameworks under the answer set semantics. 
We first introduce two of them: \textit{Cautious Induction} and \textit{Brave Induction} (\cite{Sakama2009}), 
which are the foundations of \textit{Learning from Answer Sets} discussed in Section \ref{section_lasp}, a state-of-the-art ILP framework that we will use for our new framework (for other non-monotonic ILP frameworks, see \cite{Otero2001}, \cite{Inoue2014}, \cite{Corapi2012} and \cite{DeRaedt1997}).
\subsubsection{Cautious Induction}

\begin{defn}
\textit{Cautious Induction} task\footnote{This is more general definition of Cautious Induction than the one defined in \cite{Sakama2009}, as the concept of negative examples was not included in the original definition.} 
is of the form $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$, where B is the background knowledge, E\textsuperscript{+} is a set of positive examples and E\textsuperscript{-} is a set of negative examples.
H $\in$ ILP\textsubscript{cautious} $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ if and only if  there is at least one answer set A of B $\cup$ H (B $\cup$ H is satisfiable) such that for every answer set A of B $\cup$ H:
\begin{itemize}
\item $\forall$ e $\in$ E\textsuperscript{+} : e $\in$ A
\item $\forall$ e $\in$ E\textsuperscript{-} : e $\notin$ A
\end{itemize}
\end{defn}

\begin{examp} \normalfont (Cautious Induction) \\
\label{cautious_induction_example}

B = $\begin{cases}
	\textit{exercises}  \leftarrow \textit{not \ eat\_out.} \\
	\textit{eat\_out} \leftarrow \textit{exercises.} \\
      \end{cases}$
E\textsuperscript{+} = \{tennis\},      E\textsuperscript{-} = \{eat\_out \} \\

One possible  H $\in$ ILP\textsubscript{cautious} is \{tennis$ \leftarrow$ exercises, $\leftarrow$ not tennis \}.
\end{examp}

The limitation of Cautious Induction is that positive examples must be true for all answer sets and negative examples must not be included in any of the answer sets. These conditions may be too strict in some cases, and Cautious Induction is not able to accept a case where positive examples are true in some of the answer sets but not all answer sets of the program.

\begin{examp} \normalfont (Limitation of Cautious Induction) \\
\label{limitation_cautious}

B = $\begin{cases}
	\textit{1\{situation(P, awake), situation(P, sleep)\}1} \leftarrow \textit{person(P).} \\
	\textit{person(john).}
      \end{cases}$ \\

Neither of \textit{situation(john, awake)} nor \textit{situation(john, sleep)} is false in all answer sets. 
In this example, the hypothesis only contains person(john). Thus no examples could be given to learn the choice rule.
\end{examp}

\subsubsection{Brave Induction}
\begin{defn} \label{def:brave_induction}
\textit{Brave Induction} task is of the form $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ where, B is the background knowledge, E\textsuperscript{+} is a set of positive examples and E\textsuperscript{-} is a set of negative examples.
 H $\in$ ILP\textsubscript{brave} $\langle$ B, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ if and only if there is at least one answer set A of B $\cup$ H such that:
\begin{itemize}
\item $\forall$ e $\in$ E\textsuperscript{+} : e $\in$ A
\item $\forall$ e $\in$ E\textsuperscript{-} : e $\notin$ A
\end{itemize}
\end{defn}

\begin{examp} \normalfont (Brave Induction) \\

B = $\begin{cases}
	\textit{exercises}  \leftarrow \textit{not \ eat\_out.} \\
	\textit{tennis} \leftarrow \textit{holiday} \\
      \end{cases}$
E\textsuperscript{+} = \{tennis\},   E\textsuperscript{-} = \{eat\_out \} \\

One possible  H $\in$ ILP\textsubscript{brave} is \{tennis\}, which returns \{tennis, holiday, exercises\} as answer sets.
\end{examp}
\label{brave_induction_example}

The limitation of Brave Induction is that it is not possible to learn constraints, since the conditions of the Definition \ref{def:brave_induction} only apply to at least one answer set A, 
whereas constrains must rule out all answer sets that meet  the conditions of the Brave Induction.

\begin{examp} \normalfont (Limitation of Brave Induction) \\

B = $\begin{cases}
	\textit{1\{situation(P, awake), situation(P, sleep)\}1} \leftarrow \textit{person(P).} \\
	\textit{person(C)} \leftarrow \textit{super\_person(C).} \\
	\textit{super\_person(john).}
	\end{cases}$ \\

In order to learn the  constraint hypothesis H = \{ $\leftarrow$ not situation(P, awake), super\_person(P)\}, it is not possible to find an optimal solution.
\end{examp}
\label{limitation_brave}

\subsection{Inductive Learning of Answer Set Programs (ILASP)}
\label{section_lasp}

\subsubsection{Learning from Answer Sets (LAS)}
\textit{Learning from Answer Sets (LAS)} was developed in \cite{Law2014} to facilitate more complex learning tasks that neither Cautious Induction nor Brave Induction could learn.
We define examples used in LAS are a set of atoms called a \textit{Partial Interpretation}.
\begin{defn}
A \textit{Partial Interpretation E} is of the form:
\begin{equation}
\begin{split}
E = \langle e\textsuperscript{inc}, e\textsuperscript{exc} \rangle. 
\end{split}
\label{partial_interpretations}
\end{equation}
where e\textsuperscript{inc} and e\textsuperscript{exc} are called \textit{inclusions} and \textit{exclusions} of \textit{E} respectively.
An answer set \textit{A extends E} if and only if $(e\textsuperscript{inc} \subseteq A) \wedge (e\textsuperscript{exc} \cap A = \emptyset)$.
\end{defn}

A \textit{Learning from Answer Sets} task is of the form:
\begin{equation}
\begin{split}
T = \langle B, S\textsubscript{M}, E\textsuperscript{+}, E\textsuperscript{-} \rangle
\end{split}
\label{las}
\end{equation}
where B is background knowledge, S\textsubscript{M} is hypothesis space, and E\textsuperscript{+} and E\textsuperscript{-} are examples of positive and negative partial interpretations. 
S\textsubscript{M} consists of a set of normal rules, choice rules and constraints. 
S\textsubscript{M} is specified by \textit{language bias} of the learning task using \textit{mode declaration}. 
Mode declaration specifies what can occur in a hypothesis by specifying the predicates, and consists of two parts: \textit{modeh} and \textit{modeb}. 
\textit{modeh} and \textit{modeb} are the predicates that can occur in the head of the rule and body of the rule respectively. 

\begin{defn}{Learning from Answer Sets (LAS)}

Given a learning task T, the set of all possible inductive solutions of T, denoted ILP\textsubscript{LAS}(T), and a hypothesis H is an inductive solution of ILP\textsubscript{LAS}(T) $\langle$ B, S\textsubscript{M}, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$ such that:
\begin{enumerate}
\item H $\subseteq$ S\textsubscript{M}
\item $\forall$ e\textsuperscript{+} $\in$ E\textsuperscript{+} : $\exists$ A $\in$ Answer Sets(B $\cup$ H) such that A extends e\textsuperscript{+}
\item $\forall$ e\textsuperscript{-} $\in$ E\textsuperscript{-} : $\nexists$ A $\in$ Answer Sets(B $\cup$ H) such that A extends e\textsuperscript{-}
\end{enumerate}
\end{defn}

\begin{examp} \normalfont (LAS).
Suppose a learning task T is as follows:
\begin{flalign*}
\label{appendix:example}
&\textsf{\#modeh(1, drive(var(person))).}&\\
&\textsf{\#modeb(1, hasCar(var(person))).}&\\
&\textsf{\#pos(e1, \{hasCar(john), drive(john)\}, \{\}})&\\
&\textsf{\#pos(e2, \{hasCar(anna), drive(anna)\}, \{\}})&\\
&\textsf{person(john).}&\\
&\textsf{person(anna).}
\end{flalign*}
where, both \textsf{person(john)} and \textsf{person(anna)} are background knowledge B, \textsf{\#modeh} and \textsf{\#modeb} are language bias SM and two positive examples \textsf{\#pos} are given.

\end{examp}

\subsubsection{Inductive Learning of Answer Set Programs (ILASP)}

\textit{Inductive Learning of Answer Set Programs (ILASP)} is an algorithm that is capable of solving a LAS task ILP\textsubscript{LAS}(T) $\langle$ B, S\textsubscript{M}, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$, 
and is based on two fundamental concepts: \textit{positive solutions} and \textit{violating solutions}.

A hypothesis H is a positive solution if and only if
\begin{enumerate}
\item H $\subseteq$ S\textsubscript{M}
\item $\forall$ e\textsuperscript{+} $\in$ E\textsuperscript{+} $\exists$ A $\in$ Answer Sets(B $\cup$ H) such that A extends e\textsuperscript{+}
\end{enumerate}
A hypothesis H is a violating solution if and only if
\begin{enumerate}
\item H $\subseteq$ S\textsubscript{M}
\item $\forall$ e\textsuperscript{+} $\in$ E\textsuperscript{+} $\exists$ A $\in$ Answer Sets(B $\cup$ H) such that A extends e\textsuperscript{+}
\item $\exists$ e\textsuperscript{-} $\in$ E\textsuperscript{-} $\exists$ A $\in$ Answer Sets(B $\cup$ H) such that A extends e\textsuperscript{-}\\
\end{enumerate}

Given both definitions of positive and violating solutions, ILP\textsubscript{LAS} $\langle$ B, S\textsubscript{M}, E\textsuperscript{+}, E\textsuperscript{-} $\rangle$is positive solutions that are not violating solutions.

\subsubsection{Context-dependent Learning from Answer Sets }
\textit{Context-dependent learning from answer sets ($ILP_{LAS}^{context}$)} is a further generalisation of ILP\textsubscript{LAS} with \textit{context dependent examples}\cite{Law2016}\footnote{The original paper developed the framework called \textit{learning from ordered answer sets ($ILP_{LOAS}^{context}$)}. 
In this paper, we do not use ordered answer sets and therefore simplify it to $ILP_{LAS}^{context}$. }.
\textit{Context-dependent examples} are examples in which each unique background knowledge, or \textit{context}, only applies to specific examples. 
This way the background knowledge is more structured rather than one fixed background knowledge that is applied to all examples.

Formally, partial interpretation is called \textit{context-dependent partial interpretation (CDPI)}, which is of the form:
\begin{equation} \label{eq:cdpi}
\begin{split}
\langle e, C \rangle
\end{split}
\end{equation}

where \textit{e} is a partial interpretation and C is a context, or an ASP program without weak constraints.

%TODO weak delete.

%A \textit{context-dependent ordering example (CDOE)} is of the form $\langle$ $\langle$e\textsubscript{1}, C\textsubscript{1} $\rangle$, $\langle$ e\textsubscript{2}, C\textsubscript{2} $\rangle$$\rangle$, which is a pair of CDPI. 
%An APS program P \textit{bravely respects o} if and only if
%\begin{enumerate}
% \item $\exists$ $\langle$ A\textsubscript{1}, A\textsubscript{2} $\rangle$ such that A\textsubscript{1} $\in$ Answer Sets(P $\cup$ C\textsubscript{1}),  A\textsubscript{2}, $\in$ Answer Sets(P $\cup$ C\textsubscript{2}), A\textsubscript{1} extends e\textsubscript{1}, A\textsubscript{2} extends e\textsubscript{2} and A\textsubscript{1} $\prec$\textsubscript{P} A\textsubscript{2}
%\end{enumerate}
%
% Similarly, an APS program P \textit{cautiously} respects \textit{o} if and only if
%\begin{enumerate}
% \item $\forall$ $\langle$ A\textsubscript{1}, A\textsubscript{2} $\rangle$ such that A\textsubscript{1} $\in$ Answer Sets(P $\cup$ C\textsubscript{1}),  A\textsubscript{2}, $\in$ Answer Sets(P $\cup$ C\textsubscript{2}), A\textsubscript{1} extends e\textsubscript{1}, A\textsubscript{2} extends e\textsubscript{2} and A\textsubscript{1} $\prec$\textsubscript{P} A\textsubscript{2}
%\end{enumerate}
\begin{defn} \label{def:las_context}
$ILP_{LAS}^{context}$ task is of the form T = $\langle$ B, S\textsubscript{M}, E\textsuperscript{+}, E\textsuperscript{-}$\rangle$ B.
A hypothesis H is an inductive solution of T if and only if:
\begin{enumerate}
\item H $\subseteq$ S\textsubscript{M}
\item $\forall$$\langle$ e, C$\rangle$ $\in$ E\textsuperscript{+}, $\exists$A $\in$ Answer Sets (B $\cup$ C $\cup$ H) such that A extends e
\item $\forall$$\langle$ e, C$\rangle$ $\in$ E\textsuperscript{-}, $\nexists$A $\in$ Answer Sets (B $\cup$ C $\cup$ H) such that A extends e
%\item $\forall$o $\in$ O\textsuperscript{b} B $\cup$ H bravely respects o
%\item $\forall$o $\in$ O\textsuperscript{c} B $\cup$ H cautiously respects o
\end{enumerate}
\end{defn}
One of the main advantages of using context dependent examples is that when a hypothesis is computed, only the relevant set of examples are used for search rather than using all examples at once. 
Relevant examples are counterexamples for the previously computed hypothesis in the previous iterations. This iterative approach increases the efficiency of solving learning tasks, and enables more expressive structure of the background knowledge to particular examples. 

$ILP_{LAS}^{context}$ is used in ILASP2i, which we use for our new framework.

%These features will be useful when a game agent is in two different environmets as discussed in Section \ref{model_generation_and_update}.

\begin{examp} \normalfont ($ILP_{LAS}^{context}$).

Suppose a learning task T is as follows:
\begin{flalign*}
\label{appendix:example}
&\textsf{\#modeh(1, drive(var(person))).}&\\
&\textsf{\#modeb(1, hasCar(var(person))).}&\\
&\textsf{\#pos(e1, \{hasCar(john), drive(john)\}, \{\}})&\\
&\textsf{\#pos(e2, \{hasCar(anna), drive(anna)\}, \{\}})&\\
&\textsf{person(john).}&\\
&\textsf{person(anna).}
\end{flalign*}
where, both \textsf{person(john)} and \textsf{person(anna)} are background knowledge B, \textsf{\#modeh} and \textsf{\#modeb} are language bias SM and two positive examples \textsf{\#pos} are given.
\end{examp}

\section{Reinforcement Learning (RL)}
\label{rl}
\textit{Reinforcement learning (RL)} is a subfield of machine learning regarding how an agent behaves in an environment in order to maximise its total reward. 
As shown in Figure \ref{agent_env}, the agent interacts with an environment, and at each time step the agent takes an action and receives observation, which affects the environment state and the reward (or penalty) it receives as the action outcome. 
In this section, we briefly introduce the background of RL necessary for our new framework as well as benchmarks of the experiments discussed in Chapter \ref{evaluation}.

\begin{figure}[!htb]
\centering
\includegraphics[width=6cm, height=6cm]{./figures/agent_env}
\caption{The interaction between an agent and an environment}
\label{agent_env}
\end{figure}

\subsection{Markov Decision Process (MDP)}
\label{mdp_subsection}
An agent interacts with an environment in a sequence of discrete time steps, which is part of the sequential history of observations, actions and rewards. The sequential history is formalised as 
\begin{equation}
\begin{split}
H\textsubscript{t} = S\textsubscript{1}, R\textsubscript{1}, A\textsubscript{1}, ..., A\textsubscript{t-1}, S\textsubscript{t}, R\textsubscript{t}.  
\end{split}
\end{equation}
where S is \textit{states}, R is \textit{rewards} and A is \textit{actions}. A state S\textsubscript{t} determines the next environment and has a \textit{Markov property} if and only if 
\begin{equation}
\begin{split}
P[S\textsubscript{t+1} \vert S\textsubscript{t}] = P[S\textsubscript{t+1} \vert S\textsubscript{1}, ..., S\textsubscript{t}].
\end{split}
\end{equation}
In other words, the probability of reaching S\textsubscript{t+1} depends only on S\textsubscript{t}, which captures all the relevant information from the earlier history (\cite{Puterman1994}).
When an agent must make a sequence of decision, the sequential decision problem can be formalised using the \textit{Markov decision process (MDP)}. MDP formally represents a fully observable environment of an agent for 
RL.
\begin{defn}{Markov Decision Process (MDP)}

\textit{Markov decision process (MDP)} is defined in the form of a tuple $\langle$S, A, T, R$\rangle$ where:
\begin{itemize}
\item S is the set of finite states that is observable in the environment.
\item A is the set of finite actions taken by the agent.
\item T is \textit{a state transition function}: S $\times$ A $\times$ S $\rightarrow$ [0,1], which is a probability of reaching a future state s$^\prime$ $\in$ S by taking an action a $\in$ A in the current state  s $\in$ S.
%\item T\textsubscript{a}(s, s$^\prime$) is a state transition in the form of probability matrix Pr(S\textsubscript{t+1} = s$^\prime$ $\vert$ s\textsubscript{t} = s, a\textsubscript{t} = a), which is the probablity that action \textit{a} in state \textit{s} at time \textit{t} will result in state \textit{s$^\prime$} at time \textit{t+1}.
\item R is a reward function R\textsubscript{a}(s, s$^\prime$) = $\displaystyle \E[R\textsubscript{t+1} $ $\vert$ S\textsubscript{t} = s, A\textsubscript{t} = a], the expected immediate reward that action \textit{a} in state \textit{s} at time \textit{t} will return.
\end{itemize}
\end{defn}
%$\displaystyle \E[R\textsubscript{t+1} \vert$ S\textsubscript{t} = s]

The objective of an agent is to solve an MDP by taking a sequence of actions to maximise the total cumulative reward it receives. 
A method of finding the optimal solution of an MDP is Reinforcement Learning (RL). 
Formally, the objective of a RL agent is to maximise \textit{expected discounted return},which is defined as:
\begin{equation}
\begin{split}
G\textsubscript{t} = R\textsubscript{t+1} + \gamma R \textsubscript{t+2} + ...  = \sum_{k=0}^{\infty} \gamma\textsuperscript{k} R\textsubscript{t+k+1} 
\end{split}
\end{equation}

where $\gamma$ is a discount rate $\gamma$ $\in$ [0,1], a parameter which represents the preference of the agent for the present reward over future rewards. If $\gamma$ is low, the agent is more interested in maximising immediate rewards. 

% TODO explain what k is 
For RL programs, it is not necessary for the agent to know T and R in advance, but they are present in the environment and can be realised each time the agent takes an action. 

\subsection{Policies and Value Functions}
\label{policy_value_functions_subsection}
Most RL algorithms are concerned with estimating \textit{Value functions}.
Value functions estimate the expected return, or expected future reward,  for a given action in a given state. The expected reward for an agent is dependent on the agent's action.  
Value functions are defined by \textit{policies}, which maps from states to probabilities of choosing each action. The state value function v\textsubscript{$\pi$}(s) of an MDP under a policy $\pi$ is the expected return starting from state \textit{s}, which is of the form:

\begin{equation}
v\textsubscript{$\pi$}(s) = \displaystyle \E [G\textsubscript{t} \vert S\textsubscript{t} = s] \ \textsf{for all}\ s \in \mathcal{S}
\end{equation}

The optimal state-value function v\textsuperscript{*}(s) maximises the value function over all policies in the MDP, which is of the form:

\begin{equation}
v\textsuperscript{*}(s) = \underset{\pi}{\max} \ v\textsubscript{$\pi$}(s)
\end{equation}

\begin{examp} \normalfont (Value Functions).

XXX
\end{examp}

Similar to the state value function v\textsubscript{$\pi$},  we define an \textit{action-value function} under a policy $\pi$, which represents the value of taking action a in state s following a policy $\pi$. 
\begin{equation}
q\textsubscript{$\pi$}(s, a) = \displaystyle \E [G\textsubscript{t} \vert S\textsubscript{t} = s, A\textsubscript{t} = a] \ \textsf{for all}\ s \in \mathcal{S}
\end{equation}

The optimal state-action-value function q\textsuperscript{*}(s,a) maximises the action-value function over all policies in the MDP, which is of the form:

\begin{equation}
q\textsuperscript{*}(s, a) = \underset{\pi}{\max} \ q\textsubscript{$\pi$}(s, a)
\end{equation}

A solution to MDP is called a \textit{policy $\pi$}, a sequence of actions that leads to a solution. An optimal policy achieves the optimal value function (or action-value function), and it can be computed by maximising over the optimal value function (or action-value function).

\begin{examp} \normalfont (Action-value Functions).

XXX
\end{examp}

%TODO BELLMAN OPTIMALITY EQUATION

%A policy $\pi$ is optimal if it maximises the action-value function q()
%The transition and reward functions are not necessary to be known to compute $\pi$.
%Among all possible value functions, an \textit{optimal policy $\pi^*$} is the one that maximise the total rewards in the environment.
%The optimal policy $\pi$\textsuperscript{*} corresponds to v\textsuperscript{*}(s)

%\begin{equation}
%\pi \textsuperscript{*} =
%%\pi \textsuperscript{*} = arg max v\textsubscript{\pi}(s)
%\end{equation}

%Reinforcement learning is a method to get approximated optimal solution.
%TODO on-policy and off-policy learning.

%A\textsubscript{t} = $\pi$(S\textsubscript{t})
%
%One of the common possibilities is that the agent chooses an action in order to maximise the discounted sum of future rewards.
%
%A\textsubscript{t} to maximise R\textsubscript{t+1} + $\gamma$ R\textsubscript{t+2} + $\gamma^2$ R\textsubscript{t+3} + ...
%
%An action-value function evaluate a particular state by taking an action according the policy
%
%q\textsubscript{$\pi$} = $\displaystyle \E[R\textsubscript{t+1} $[R\textsubscript{t+1} + $\gamma$ R\textsubscript{t+2} + $\gamma^2$ R\textsubscript{t+3} + ... $\vert$ S\textsubscript{t} = s, A\textsubscript{t} = a, A\textsubscript{t+1 } = a,]

\subsection{Model-based and Model-free RL}
\label{model_base_model_free_subsection}
A \textit{model} is a representation of an environment that an agent can use to understand how the environment should look like. 
There are two different types of RL methods: \textit{model-based} and \textit{model-free} RL method. 
Model-based RL is where, when a model of the environment is known, the agent uses the model to plan for a series of actions to achieve the agent's goal. 
The model itself can be learnt by interacting with the environment by taking actions, which return states and rewards, and the parameters of the action models can be estimated with maximum likelihood methods \cite{Ray2010}.
Using a model, an agent can do planning to generate or improve a policy for the modeled environment. 
Most of the RL methods are model-free RL, where the model is unknown and the agent learns to achieve the goal solely by interacting with the environment. 
Thus the agent knows only possible states and actions, and the transition state and reward probability functions are unknown.
When the model of the environment is correct, unlike with model-free RL, the agent does not require trial-and-error interactions with the environment and therefore model-based RL is faster to reach an optimal policy. 
However, when the model is not a true representation of the environment, or the true MDP, the planning algorithms will not lead to the optimal policy, but a suboptimal policy.

One algorithm which combines both aspects of model-based and model-free learning to solve the issue of sub-optimality is called Dyna \cite{Sutton1990}.
Dyna learns a model from real experience and uses the model to generate simulated experience to update the evaluation functions.
This approach is more efficient because the simulated experiences are relatively easy to generate compared to real experiences, thus less iterations are required.

This idea of learning a model of the environment and using it to execute planning is a core idea of our new framework. 

\subsection{Temporal-Difference (TD) Learning}
\label{td_learning_section}

One of the RL approaches for solving a MDP is called \textit{Temporal-Difference (TD) Learning}.
TD learning is an online model-free RL which learns directly from episodes of incomplete experiences without a model of the environment.
TD updates the estimates of value function as follows:

\begin{equation}
\centering
V(S\textsubscript{t}) \leftarrow V(S\textsubscript{t}) + \alpha[R\textsubscript{t+1} + \gamma V(S\textsubscript{t+1}) - V(S\textsubscript{t})]
\end{equation}

where R\textsubscript{t+1} + $\gamma$ V(S\textsubscript{t+1}) is the target for TD update, which is a biased estimate of v\textsubscript{$\pi$} (S\textsubscript{t}), and $\delta$ = R\textsubscript{t+1} + $\gamma$ V(S\textsubscript{t+1}) - V(S\textsubscript{t}) is called \textit{TD error}, which is the error in V(S\textsubscript{t}) available at time t+1.
Since the TD method only needs to know the estimate of one step ahead and does not need the final outcome of the episodes (also called TD(0)), it can learn online after every time step. TD also works without the terminal state, which is the goal for an agent.
TD(0) is proved to converge to v\textsubscript{$\pi$} in the table-based case (non-function approximation).
%However, because bootstraping updates an estimate for an estimate, some bias are inevitable.
%In addition, TD method is sensitive to initial value, (but low variance).

%\begin{equation}
%\textbf{	V(S\textsubscript{t} \leftarrow V(S\textsubscript{t} + \alpha (R\textsubscript{t+1} + \gamma V(S\textsubscript{t+1}) - V(S\textsubscript{t}))
%}\end{equation}

\textit{Q-learning} is off-policy TD learning defined in \cite{Watkins}, where the agent only knows about the possible states and actions. 
The transition states and reward probability functions are unknown to the agent.
The update rule for the state-action value function, called \textit{Q function}, is of the form:

\begin{equation}\label{eq:q_learning}
Q(s\textsubscript{t},a\textsubscript{t}) \leftarrow Q(s\textsubscript{t},a\textsubscript{t}) +  \alpha(R\textsubscript{t+1} + \gamma  max (a+t) Q(s\textsubscript{t+1}, a\textsubscript{t+1}) - Q(s\textsubscript{t}, a\textsubscript{t}))
\end{equation}

where $\alpha$ is a constant step-size parameter, or learning rate, $\alpha$ between 0 and 1,  and $\gamma$ is a discount rate. 
The function Q(S,A) predicts the best action A in state S to maximise the total cumulative rewards.

%\begin{algorithm}
%\caption{Q-learning (off-policy TD control)}\label{euclid}
%\begin{algorithmic}[1]
%\Procedure{}{}
%\State $\text{Initialise Q(s,a) arbitrarily, and Q(terminal-state,.)  = 0}$
%%\For
%%$\text{Repeat (for each episode):}$
%\State $\text{Choose a from s using policy derived from Q (e.g, epsilon-greedy)}$
%\State $\text{Take action a, observe r,  s$\prime$}$

%\begin{equation}
%Q(s,a) \leftarrow Q(s,a) +  \alpha(R\textsubscript{t+1} + \gamma  max (a+t) Q(s$\prime$, a$\prime$) - Q(s\textsubscript{t}, a\textsubscript{t}))
%s \gets s$\prime$
%\end{equation}

%\EndProcedure
%\caption{Q-learning for estimating $\pi$ $\approx$ $\pi$\textsubscript{*} }
%\end{algorithmic}
%\end{algorithm}

\begin{equation}
Q(s\textsubscript{t},a\textsubscript{t}) = E [R\textsubscript{t+1} + \gamma R\textsubscript{t+2} + \gamma^2 R\textsubscript{t+3} + ... \vert s\textsubscript{t},a\textsubscript{t} ]
\end{equation}

Q-learning is guaranteed to converge to an optimal policy in a finite tabular representation.
Since Q-learning is one of the most widely used RL methods and our experiments are conducted in a tabular representation, we use it as one of the benchmarks for our new framework.
%\textcolor{red}{Paper Jaakkola et al. 1993}

%The optimal Q-function Q\textsuperscript{*}(s,a) is directly approximated by the learned action-value function Q.

% Model free can be done using Monte Carlo Policy evaluation
% One way to solve the Bellman Optimality equation is Q-leraning
% U(s) = max a Q(s,a)
% The function is estimated by Q-learning, which repeately updates Q(s,a) using the Bellman Equation.
%Q-learning learns the value of its deterministic greedy policy from the experience and gradually converge to the optimal Q-function. It also explored following \textit{$\epsilon$-greedy policy}, which is a stochastic greedy policy, but with the probability of $\epsilon$, the agent chooses an action randomly instea of the greedy action.

\subsection{Function Approximation}
\label{function_approximation}
Q-learning with a tabular representation works when every state-action value can be represented. 
In case of very large MDPs, however, it may not be possible to represent all state-action values with a tabular representation.
For example, robot arms have a continuous states in 3D dimensional space.
This problem motivates the use of \textit{function approximation}, which estimates value function with function approximation.  Not only is it represented in tabular form, but also in the form of a parameterised function with \textit{weight vector w}.
%$\in \R\textsuperscript{d}$ where $\R\textsuperscript{d}$ is XXX
Unlike Q-table, changing one weight updates the estimated value of not only one state, but many states, and this generalisation makes it more flexible to apply to different scenarios where the tabular approach could not be applied.

The reason we are introducing the function approximation is that it is used as another benchmark for our new framework.

\subsubsection{The Prediction Objective ($\overline{VE}$)}
With function approximation, an update at one state changes many other states, and therefore the values of all states will not be exactly accurate.
There is a tradeoff among states as to which state we make it more accurate, while others might be less accurate.
The error in a state s is the square of the difference between the approximate value $\hat{v}$(s,w) and the true value v\textsubscript{$\pi$}(s). The objective function can be defined by weighting it over the state space by $\mu$, the \textit{Mean Squared Value Error}, denoted $\overline{VE}$.

\begin{equation}
\overline{VE}(w) \doteq \sum_{s \in S} \mu (s) \big[ v\textsubscript{$\pi$}(s) - \hat{v}(s,w) \big]\textsuperscript{2}.
\end{equation}
\label{ve}

\subsubsection{Stochastic gradient descent (SGD)}
\textit{Stochastic gradient descent (SGD)} methods are commonly used to learn function approximation in value prediction, which works well for online reinforcement learning.
%TODO EXPLAIN ONLINE VS OFFLINE LEARNING
The weight vector w in SGD is defined as:
\begin{equation}
 w \doteq  \begin{pmatrix}  w\textsubscript{1}  \\ w\textsubscript{2} \\ \vdots \\ w\textsubscript{n}   \end{pmatrix}
\end{equation}

and $\hat{v}$(s,w) is a differentiable function of w for all s $\in$ S.
%minimize the $\overline{VE}$ on the observed examples. 
SGD adjusts the weights vector by a fraction of $\alpha$, a step-size parameter,  in the direction that reduces the error on that example the most. Formally, it is defined as:

\begin{equation}\label{eq:sgd}
\begin{split}
w\textsubscript{t+1} & \doteq w\textsubscript{t} -  \frac{1}{2} \alpha  \bigtriangledown \big[ v\textsubscript{$\pi$}(S\textsubscript{t}) - \hat{v}(S\textsubscript{t}, w\textsubscript{t}) \big]\textsuperscript{2}. \\
& = w\textsubscript{t} +  \alpha  \big[ v\textsubscript{$\pi$}(S\textsubscript{t}) - \hat{v}(S\textsubscript{t}, w\textsubscript{t}) \big] \bigtriangledown \hat{v}(S\textsubscript{t}, w\textsubscript{t}).
\end{split}
\end{equation}

The gradient of J(w), which is the vector of partial derivatives with respect to each weight vector,  is defined as:

\begin{equation}
\bigtriangledown\textsubscript{w} J(w) =  \begin{pmatrix} \frac{\partial J(w)}{\partial w\textsubscript{1}}  \\ \vdots \\ \frac{\partial J(w)}{\partial w\textsubscript{n}}   \end{pmatrix}
\end{equation}

%\begin{equation}
%w\textsubscript{t+1} \doteq w\textsubscript{t} + \alpha \big[ U\textsubscript{t} - \hat{v}(S\textsubscript{t}, w\textsubscript{t})\big] x(S\textsubscript{t})
%\end{equation}

\subsubsection{Linear Value Function Approximation}
A \textit{linear function} of a weight vector is a type of simple function approximation to approximate the action-value function. 
\begin{equation}
\hat{q}(s,a) \approx q\textsubscript{$\pi$} (s,a)
\end{equation}

The vector x(s,a) represents a \textit{feature vector}, which is of the form:

\begin{equation}
x(S, A) = \begin{pmatrix} x\textsubscript{1}(S, A) \\ \vdots \\ x\textsubscript{n}(S, A)  \end{pmatrix}
\end{equation}
where each x(s,a) is a feature of the corresponding action-state pair.

Using the SGD update with linear function approximation, the gradient of the approximate value function with respect to w is:
%\textcolor{red}{Add proof here}
\begin{equation}
\bigtriangledown \hat{q}(s, a,w) = x(s, a)
\end{equation}

Thus the general SGD update defined in (\ref{eq:sgd}) can be simplified to the following:

\begin{equation}\label{eq:sgd_linear}
\begin{split}
w\textsubscript{t+1} & = w\textsubscript{t} +  \alpha  \big[ q\textsubscript{$\pi$}(S\textsubscript{t}, A\textsubscript{t}) - \hat{q}(S\textsubscript{t}, A\textsubscript{t}, w\textsubscript{t}) \big] x(S\textsubscript{t}, A\textsubscript{t})
\end{split}
\end{equation}

%The error in a state s is the square of the difference between the approximate value
%$\hat{v}$(S,w) and the true value v(S,w).
%\begin{equation}
%J(w) = \displaystyle \E \textsubscript{$\pi$} \big[ (v\textsubscript{$\pi$}(S) -  \hat{v}(S,w))\textsuperscript{2} \big]
%\end{equation}
Unlike non-linear value function approximation, the linear method is guaranteed to converge to a global optimum.
One disadvantage of the linear method is that it cannot express any relationship between features. For example, it cannot represent that feature \textit{i} is useful only if feature \textit{j} is not present.
Nevertheless,  this approach is sufficient for our experiments, which are described in Chapter \ref{evaluation}.

\subsubsection{Tile Coding \textcolor{red}{(TBD)}}
There are different linear function approximation methods to represents states as features. Feature construction depends on a problem you are solving.  We introduce \textit{Tile Coding} which will be used for our benchmark.
\begin{figure}[!htb]
\centering
\includegraphics[width=1.0\textwidth]{./figures/tile_coding}
\caption{Tiling coding illustration}
\label{fig:tile_coding}
\end{figure}

Illustration of tile coding is shown in Figure \ref{fig:tile_coding}. In Tile Coding, state set is represented as a continuous two-dimentionala space. If a state is within the space, then the value of the  corrensponding feature is set to be 1 to indicate that the feature is present, while 0 indicates that the feature is absent. This way of representing the feature is called \textit{binary feature}.  \textit{Coarse coding} represents a state with which binary features are present within the space.
One area is associated with one weight w, and training at a state will affect the weight of all the areas overlapping that state. the approximate value function will be updated within at all states within the union of the areas, and a point that has more overlap will be more affected.
The size and shape of the areas will determine the degree of the generalisation. Large areas will have more generalisation. In addition, tiles can be overlap, and the change of the weight in that state will affect all other states within the intersection of the spaces. 
The degree of overlap within a space will determined the degree of the generalisation.
The shape of the space also affect how it is generalised.

\textit{Tile coding} is a type of coarse coding. \textit{Tiling} is a partition of state space, and each element of the partition is called a \textit{tile}.

The state stpace is partitioned into multiple tiles with multiple tilings. Each tile in each tiling is associated with

In order to do coarse coding with tile coding, multiple tilings are required, each tiling is offset from one another by a fraction of a tile width.

As illustrated in Figure XXX, when a state occurs, several features with corresponding tiles become active,

Tile coding has computational advantage, since each component of tiling is binary value,  XXXX.

a trained state will be generalised to other states if they are within any of the same tiles.

Similar to coarse coding, the size and shape of tiles will determine the degree of approximation.

\begin{examp} \normalfont (Tile Coding).

XXX
\end{examp}

\subsection{Transfer Learning \textcolor{red}{(TBD)}}
\label{transfer_learning}

Transfer learning is a method that knowledge learnt in one or more tasks can be used to learn a new task better than if without the knowlege in the first task.

Transfer learning is an active research areas in machine learning, but not many have been done in RL.
Since training tend to be time consuming and computational expensive, transfer learning allow the trained model to be applied in a different setting.

Transfer learning in RL is particularly important since most of the RL research have been done in a simulation or game scenarios, and training RL models in a real physical environment is more expensive to conduct.

Even in a virtual environments like games,  the transfer learning between different tasks will greatly will have a big impact on potential applications.

This will also speed up learning

Transfer learning in ILP domain have been proved to be successful in many fields,

Since this project is combining ILP into RL senarios, this has a potential for extending this particular research.

We conducted experiements on transfer learning capabilities, which we describe in XXX.

One of the  purposes of transfer learning is so that the agent requires less time to learn a new task with the help of what was learn in previous tasks.


Another goal would be to measure how effectively the agent reuses its knoledge in a new task.
In this case the performan of learningon the first task is usally not measured.

There are many different matrices used to measure the performance of the transfer learning.
Five common matrics are defined in XX as follow.

TODO source task selection

%Compare with human play, which is used in DeepMind Atari paper.
%
%\begin{itemize}
%\item Jumpstart
%\item Asymptotic Performance
%\end{itemize}

Since each matric measures different aspect of transfer learning, using multiple metrics would provide more comprehensive views of the performance of an RL algorithm.
%
%\begin{equation}
%\begin{split}
%r = \frac{Area under curve with transfer - area under curve without transfer}{area under curve without transfer}
%\end{split}
%\end{equation}
%REFERENCE

\chapter{Framework}
\label{framework}
\input{chapters/framework}

\chapter{Evaluation}
\label{evaluation}
\input{chapters/evaluation}

\chapter{Related Work}
\label{related_work}
\input{chapters/related_work}

\chapter{Conclusion}
\label{conclusion}
\input{chapters/conclusion}

\newpage
\bibliography{references}
\bibliographystyle{ieeetr}

\newpage
\appendix
\chapter{Ethics}

\input{chapters/appendix_ethics}

\newpage
\chapter{Learning tasks}
\input{chapters/appendix_learning_task}

\newpage
\chapter{Answer Set Program}
\input{chapters/appendix_asp}

\end{document}
