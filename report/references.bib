@article{Lifschitz2008,
abstract = {Answer set programming (ASP) is a form of declarative pro-gramming oriented towards difficult search problems. As an outgrowth of research on the use of nonmonotonic reason-ing in knowledge representation, it is particularly useful in knowledge-intensive applications. ASP programs consist of rules that look like Prolog rules, but the computational mech-anisms used in ASP are different: they are based on the ideas that have led to the creation of fast satisfiability solvers for propositional logic.},
author = {Lifschitz, Vladimir},
file = {:home/kiyo/Dropbox/Individual Project/ILP/What is ASP.pdf:pdf},
journal = {Aaai},
keywords = {Senior Member Papers},
pages = {1594--1597},
title = {{What Is Answer Set Programming?.}},
volume = {8},
year = {2008}
}

@inproceedings{gringo,
abstract = {We describe the major new features emerging from a significant redesign of the grounder gringo, building upon a grounding algorithm based on semi-naive database evaluation. Unlike previous versions, rules only need to be safe rather than domain-restricted.},
address = {Berlin, Heidelberg},
author = {Gebser, Martin and Kaminski, Roland and K{\"{o}}nig, Arne and Schaub, Torsten},
booktitle = {Logic Programming and Nonmonotonic Reasoning},
editor = {Delgrande, James P and Faber, Wolfgang},
isbn = {978-3-642-20895-9},
pages = {345--351},
publisher = {Springer Berlin Heidelberg},
title = {{Advances in gringo Series 3}},
year = {2011}
}


@article{Ray2010,
abstract = {Model-based Reinforcement Learning refers to learning optimal behavior indirectly by learning a model of the environment by taking actions and observing the outcomes that include the next state and the immediate reward. The models predict the outcomes of actions and are used in lieu of or in addition to interaction with the environment to learn optimal policies. 3 Motivation and Background Reinforcement Learning (RL) refers to learning to behave optimally in a stochastic environment by taking actions and receiving rewards [1]. The environment is assumed Markovian in that there is a fixed probability of the next state given the current state and the agent's action. The agent also receives an immediate reward based on the current state and the action. Models of the next-state distribution and the immediate rewards are referred to as " action models " and, in general, are not known to the learner. The agent's goal is to take actions, observe the outcomes including rewards and next states, and learn a policy or a mapping from states to actions that optimizes some performance measure. Typically the performance measure is the expected total reward in episodic domains, and the expected average reward per time step or expected discounted total reward in infinite-horizon domains. The theory of Markov Decision Processes (MDPs) implies that under fairly general conditions, there is a stationary policy, i.e., a time-invariant mapping from states to actions, that maximizes each of the above reward measures. Moreover, there are MDP solution algorithms, e.g., value iteration and policy iteration [2], which can be used to solve the MDP exactly given the action models. Assuming that the number of states is not exceedingly high, this suggests a straight-forward approach for model-based reinforcement learning. The models can be learned by interacting with the environment by taking actions, observing the resulting states and rewards, and estimating the parameters of the action models through maximum likelihood methods. Once the models are estimated to a desired accuracy, the MDP solution algorithms can be run to learn the optimal policy. One weakness of the above approach is that it seems to suggest that a fairly accurate model needs to be learned over the entire domain to learn a good policy. Intuitively it seems that we should be able to get by without learning highly accurate models for suboptimal actions. A related problem is that the method does not suggest how best to explore the domain, i.e., which states to visit and which actions to execute to quickly learn an optimal policy. A third issue is one of scaling these methods, including model learning, to very large state spaces with billions of states. The remaining sections outline some of the approaches explored in the literature to solve these problems.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Ray, Soumya and Tadepalli, Prasad},
doi = {10.1016/j.brainres.2010.09.091},
eprint = {1603.02199},
file = {:home/kiyo/Downloads/model-based.pdf:pdf},
isbn = {978-0-387-30164-8},
issn = {18726240},
journal = {Encyclopedia of Machine Learning},
pages = {690--693},
pmid = {20920485},
title = {{Model-Based Reinforcement Learning}},
url = {http://dx.doi.org/10.1007/978-0-387-30164-8{\_}556},
year = {2010}
}

@article{Brockman2016,
archivePrefix = {arXiv},
arxivId = {1606.01540},
author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
eprint = {1606.01540},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brockman et al. - 2016 - OpenAI Gym.pdf:pdf},
month = {jun},
title = {{OpenAI Gym}},
url = {http://arxiv.org/abs/1606.01540},
year = {2016}
}
@article{Law2016b,
abstract = {In recent years, several frameworks and systems have been proposed that extend Inductive Logic Programming (ILP) to the Answer Set Programming (ASP) paradigm. In ILP, examples must all be explained by a hypothesis together with a given background knowledge. In existing systems, the background knowledge is the same for all examples; however, examples may be context-dependent. This means that some examples should be explained in the context of some information, whereas others should be explained in different contexts. In this paper, we capture this notion and present a context-dependent extension of the Learning from Ordered Answer Sets framework. In this extension, contexts can be used to further structure the background knowledge. We then propose a new iterative algorithm, ILASP2i, which exploits this feature to scale up the existing ILASP2 system to learning tasks with large numbers of examples. We demonstrate the gain in scalability by applying both algorithms to various learning tasks. Our results show that, compared to ILASP2, the newly proposed ILASP2i system can be two orders of magnitude faster and use two orders of magnitude less memory, whilst preserving the same average accuracy. This paper is under consideration for acceptance in TPLP.},
archivePrefix = {arXiv},
arxivId = {1608.01946},
author = {Law, Mark and Russo, Alessandra and Broda, Krysia},
eprint = {1608.01946},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Law, Russo, Broda - 2016 - Iterative Learning of Answer Set Programs from Context Dependent Examples(3).pdf:pdf},
month = {aug},
title = {{Iterative Learning of Answer Set Programs from Context Dependent Examples}},
url = {http://arxiv.org/abs/1608.01946},
year = {2016}
}
@article{Muggleton1993,
abstract = {Inductive Logic Programming (ILP) is a research area which investigates the construction of quantified definite clause theories from examples and background knowledge. ILP systems have been applied successfully in a number of real-world domains. These include the learning of structure-activity rules for drug design, finite-element mesh design rules, rules for primary-secondary prediction of protein structure and fault diagnosis rules for satellites. There is a well established tradition of learning-in-the-limit results in ILP. Recently some results within Valiant's PAC-learning frame-work have also been demonstrated for ILP systems. In this paper it is argued that algorithms can be directly derived from the formal specifications of ILP. This provides a common basis for Inverse Resolution, Explanation-Based Learning, Abduction and Relative Least General Generalisation. A new general-purpose, efficient approach to predicate invention is demonstrated. ILP is underconstrained by its logical specification. Therefore a brief overview of extra-logical constraints used in ILP systems is given. Some present limitations and research directions for the field are identified.},
author = {Muggleton, Stephen},
doi = {10.1007/3-540-56602-3_125},
isbn = {9783540566021},
issn = {0163-5719},
journal = {SIGART Bulletin},
pages = {1--5},
title = {{Inductive Logic Programming: derivations, successes and shortcomings}},
volume = {5},
year = {1993}
}
@article{Sridharan2017,
abstract = {This paper describes an architecture for an agent to learn and reason about affordances. In this architecture, Answer Set Prolog, a declarative language, is used to represent and reason with incomplete domain knowledge that includes a represen-tation of affordances as relations defined jointly over objects and actions. Reinforcement learning and decision-tree induc-tion based on this relational representation and observations of action outcomes, are used to interactively and cumulatively (a) acquire knowledge of affordances of specific objects be-ing operated upon by specific agents; and (b) generalize from these specific learned instances. The capabilities of this ar-chitecture are illustrated and evaluated in two simulated do-mains, a variant of the classic Blocks World domain, and a robot assisting humans in an office environment.},
author = {Sridharan, Mohan and Meadows, Ben and Gomez, Rocio},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sridharan, Meadows, Gomez - 2017 - What can I not do Towards an Architecture for Reasoning about and Learning Affordances.pdf:pdf},
isbn = {9781577357896},
issn = {23340843},
journal = {International Conference on Automated Planning and Scheduling},
keywords = {Planning and Learning Special Track},
number = {Icaps},
pages = {461--469},
title = {{What can I not do? Towards an Architecture for Reasoning about and Learning Affordances}},
url = {http://homepages.engineering.auckland.ac.nz/{~}smohan/Papers/icaps17{\_}affordanceLearn.pdf},
year = {2017}
}
@article{Apeldoorn2016,
author = {Apeldoorn, Daan and Kern-isberner, Gabriele},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Apeldoorn, Kern-isberner - 2016 - When Should Learning Agents Switch to Explicit Knowledge.pdf:pdf},
pages = {174--186},
title = {{When Should Learning Agents Switch to Explicit Knowledge ?}},
volume = {41},
year = {2016}
}
@article{Apeldoorn,
author = {Apeldoorn, Daan and Kern-isberner, Gabriele},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Apeldoorn, Kern-isberner - Unknown - Towards an Understanding of What Is Learned Extracting Multi-Abstraction-Level Knowledge from Lear.pdf:pdf},
isbn = {9781577357872},
keywords = {Special Track on Uncertain Reasoning},
pages = {764--767},
title = {{Towards an Understanding of What Is Learned : Extracting Multi-Abstraction-Level Knowledge from Learning Agents}}
}
@article{Rafols2005,
author = {Rafols, Eddie J and Ring, Mark B and Sutton, Richard S and Tanner, Brian and Tg, Canada},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rafols et al. - 2005 - Using Predictive Representations to Improve Generalization in Reinforcement Learning.pdf:pdf},
title = {{Using Predictive Representations to Improve Generalization in Reinforcement Learning}},
year = {2005}
}
@article{Littman1992,
author = {Littman, Michael L and Sutton, Richard S},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Littman, Sutton - 1992 - Predictive Representations of State.pdf:pdf},
title = {{Predictive Representations of State}},
year = {1992}
}
@article{Zambaldi2018,
abstract = {We introduce an approach for deep reinforcement learning (RL) that improves upon the efficiency, generalization capacity, and interpretability of conventional approaches through structured perception and relational reasoning. It uses self-attention to iteratively reason about the relations between entities in a scene and to guide a model-free policy. Our results show that in a novel navigation and planning task called Box-World, our agent finds interpretable solutions that improve upon baselines in terms of sample complexity, ability to generalize to more complex scenes than experienced during training, and overall performance. In the StarCraft II Learning Environment, our agent achieves state-of-the-art performance on six mini-games -- surpassing human grandmaster performance on four. By considering architectural inductive biases, our work opens new directions for overcoming important, but stubborn, challenges in deep RL.},
archivePrefix = {arXiv},
arxivId = {1806.01830},
author = {Zambaldi, Vinicius and Raposo, David and Santoro, Adam and Bapst, Victor and Li, Yujia and Babuschkin, Igor and Tuyls, Karl and Reichert, David and Lillicrap, Timothy and Lockhart, Edward and Shanahan, Murray and Langston, Victoria and Pascanu, Razvan and Botvinick, Matthew and Vinyals, Oriol and Battaglia, Peter},
eprint = {1806.01830},
month = {jun},
title = {{Relational Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1806.01830},
year = {2018}
}
@article{Y.LeCunL.BottouY.Bengio1998,
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {{Y.LeCun, L.Bottou, Y.Bengio}, and P.Haffner},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Gradient-based learning applied to document recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
pages = {2278--2324},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@book{PutermanM.L.1994,
address = {New York, NY},
author = {{Puterman M.L.}},
publisher = {John Wiley {\&} Sons, Inc.},
title = {{Markov Decision Processes—Discrete Stochastic Dynamic Programming}},
year = {1994}
}
@misc{Watkins,
author = {Watkins},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Watkins - Unknown - Learning from Delayed Reward.pdf:pdf},
title = {{Learning from Delayed Reward}},
url = {http://www.cs.rhul.ac.uk/{~}chrisw/new{\_}thesis.pdf}
}
@article{Zambaldi2018,
abstract = {We introduce an approach for deep reinforcement learning (RL) that improves upon the efficiency, generalization capacity, and interpretability of conventional approaches through structured perception and relational reasoning. It uses self-attention to iteratively reason about the relations between entities in a scene and to guide a model-free policy. Our results show that in a novel navigation and planning task called Box-World, our agent finds interpretable solutions that improve upon baselines in terms of sample complexity, ability to generalize to more complex scenes than experienced during training, and overall performance. In the StarCraft II Learning Environment, our agent achieves state-of-the-art performance on six mini-games -- surpassing human grandmaster performance on four. By considering architectural inductive biases, our work opens new directions for overcoming important, but stubborn, challenges in deep RL.},
archivePrefix = {arXiv},
arxivId = {1806.01830},
author = {Zambaldi, Vinicius and Raposo, David and Santoro, Adam and Bapst, Victor and Li, Yujia and Babuschkin, Igor and Tuyls, Karl and Reichert, David and Lillicrap, Timothy and Lockhart, Edward and Shanahan, Murray and Langston, Victoria and Pascanu, Razvan and Botvinick, Matthew and Vinyals, Oriol and Battaglia, Peter},
eprint = {1806.01830},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zambaldi et al. - 2018 - Relational Deep Reinforcement Learning.pdf:pdf},
month = {jun},
title = {{Relational Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1806.01830},
year = {2018}
}
@article{Santoro2018,
abstract = {Memory-based neural networks model temporal data by leveraging an ability to remember information for long periods. It is unclear, however, whether they also have an ability to perform complex relational reasoning with the information they remember. Here, we first confirm our intuitions that standard memory architectures may struggle at tasks that heavily involve an understanding of the ways in which entities are connected -- i.e., tasks involving relational reasoning. We then improve upon these deficits by using a new memory module -- a $\backslash$textit{\{}Relational Memory Core{\}} (RMC) -- which employs multi-head dot product attention to allow memories to interact. Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (e.g. Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.},
archivePrefix = {arXiv},
arxivId = {1806.01822},
author = {Santoro, Adam and Faulkner, Ryan and Raposo, David and Rae, Jack and Chrzanowski, Mike and Weber, Theophane and Wierstra, Daan and Vinyals, Oriol and Pascanu, Razvan and Lillicrap, Timothy},
eprint = {1806.01822},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Santoro et al. - 2018 - Relational recurrent neural networks.pdf:pdf},
month = {jun},
title = {{Relational recurrent neural networks}},
url = {http://arxiv.org/abs/1806.01822},
year = {2018}
}
@article{Law2017,
archivePrefix = {arXiv},
arxivId = {1608.01946},
author = {Law, Mark and Russo, Alessandra and Broda, Krysia},
eprint = {1608.01946},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Law, Russo, Broda - 2017 - Inductive Learning of Answer Set Programs v3.1.0 User Manual.pdf:pdf},
issn = {16113349},
title = {{Inductive Learning of Answer Set Programs v3.1.0 User Manual}},
year = {2017}
}
@article{Su2017,
abstract = {Recent research has revealed that the output of Deep Neural Networks (DNN) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution. It requires less adversarial information and can fool more types of networks. The results show that 70.97{\%} of the natural images can be perturbed to at least one target class by modifying just one pixel with 97.47{\%} confidence on average. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks.},
archivePrefix = {arXiv},
arxivId = {1710.08864},
author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Kouichi, Sakurai},
eprint = {1710.08864},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Su, Vargas, Kouichi - 2017 - One pixel attack for fooling deep neural networks.pdf:pdf},
month = {oct},
title = {{One pixel attack for fooling deep neural networks}},
url = {http://arxiv.org/abs/1710.08864},
year = {2017}
}
@article{Paradigm2018,
author = {Paradigm, The Answer-set Programming},
file = {:home/kiyo/Dropbox/Individual Project/book/ASP{\_}ch/10.0{\_}pp{\_}114{\_}130{\_}The{\_}Answer-Set{\_}Programming{\_}Paradigm.pdf:pdf},
isbn = {9781139342124},
number = {May},
pages = {114--130},
title = {{The Answer-Set Programming Paradigm}},
year = {2018}
}
@article{Sets2018,
author = {Sets, Computing Answer},
file = {:home/kiyo/Dropbox/Individual Project/book/ASP{\_}ch/11.0{\_}pp{\_}131{\_}151{\_}Algorithms{\_}for{\_}Computing{\_}Answer{\_}Sets.pdf:pdf},
isbn = {9781139342124},
number = {May},
pages = {131--151},
title = {{Algorithms for Computing Answer Sets}},
year = {2018}
}
@article{Prolog2018,
author = {Prolog, Answer Set},
file = {:home/kiyo/Dropbox/Individual Project/book/ASP{\_}ch/06.0{\_}pp{\_}11{\_}39{\_}Answer{\_}Set{\_}Prolog{\_}ASP.pdf:pdf},
isbn = {9781139342124},
number = {May},
pages = {11--39},
title = {{Answer Set Prolog (ASP)}},
year = {2018}
}
@article{Willis2002,
author = {Willis, Scott},
file = {:home/kiyo/Dropbox/Individual Project/book/ASP{\_}ch/08.0{\_}pp{\_}61{\_}85{\_}Creating{\_}a{\_}Knowledge{\_}Base.pdf:pdf},
isbn = {9781139342124},
journal = {Educational Leadership},
number = {6},
pages = {6--79},
title = {{Creating a Knowledge Base}},
volume = {59},
year = {2002}
}
@article{Pollock1999,
author = {Pollock, John L.},
doi = {10.1007/978-94-015-9204-8_4},
file = {:home/kiyo/Dropbox/Individual Project/book/ASP{\_}ch/13.0{\_}pp{\_}192{\_}215{\_}Planning{\_}Agents.pdf:pdf},
isbn = {9781139342124},
journal = {Foundations of Rational Agency},
number = {May},
pages = {63--79},
title = {{Planning Agents}},
year = {1999}
}
@article{Gelfond2014,
author = {Gelfond, Michael and Kahl, Yulia},
file = {:home/kiyo/Dropbox/Individual Project/book/ASP{\_}ch/05.0{\_}pp{\_}1{\_}10{\_}Logic-Based{\_}Approach{\_}to{\_}Agent{\_}Design.pdf:pdf},
isbn = {978-1-139-34212-4},
number = {May},
pages = {1--10},
title = {{Logic-Based Approach to Agent Design}},
url = {http://dx.doi.org/10.1017/CBO9781139342124.002{\%}0Ahttp://ebooks.cambridge.org/chapter.jsf?bid=CBO9781139342124{\&}cid=CBO9781139342124A007},
year = {2014}
}
@article{Defaults2018,
author = {Defaults, Representing},
file = {:home/kiyo/Dropbox/Individual Project/book/ASP{\_}ch/09.0{\_}pp{\_}86{\_}113{\_}Representing{\_}Defaults.pdf:pdf},
isbn = {9781139342124},
number = {May},
pages = {86--113},
title = {{Representing Defaults}},
year = {2018}
}
@article{Prolog2018a,
author = {Prolog, Answer Set},
file = {:home/kiyo/Dropbox/Individual Project/book/ASP{\_}ch/04.0{\_}pp{\_}xi{\_}xiv{\_}Preface.pdf:pdf},
isbn = {9781139342124},
number = {May},
pages = {10--13},
title = {{No Title}},
year = {2018}
}
@book{Domains2018,
author = {Domains, Modeling Dynamic},
file = {:home/kiyo/Dropbox/Individual Project/book/ASP{\_}ch/12.0{\_}pp{\_}152{\_}191{\_}Modeling{\_}Dynamic{\_}Domains.pdf:pdf},
isbn = {9781139342124},
number = {May},
pages = {152--191},
title = {{Modeling Dynamic Domains}},
year = {2018}
}
@article{Approach2018,
author = {Approach, Logic-based and Design, Agent and Agent, Intelligent and Family, Simple and Base, Knowledge and Example, An and Comment, Historical},
file = {:home/kiyo/Dropbox/Individual Project/book/ASP{\_}ch/03.0{\_}pp{\_}vii{\_}x{\_}Contents.pdf:pdf},
isbn = {9781139342124},
number = {May},
pages = {10--13},
title = {{Preface 1}},
year = {2018}
}
@article{,
file = {:home/kiyo/Dropbox/Individual Project/book/ASP{\_}ch/01.0{\_}pp{\_}i{\_}iv{\_}Frontmatter.pdf:pdf},
isbn = {9781139342124},
number = {May 2018},
pages = {10--13},
title = {{International Journal of Theory and Prac- tice of Logic Programming}},
year = {2012}
}
@article{,
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2018 - To Lara and Patrick , with love.pdf:pdf},
isbn = {9781139342124},
number = {May},
title = {{To Lara and Patrick , with love}},
year = {2018}
}
@article{Prolog2018b,
author = {Prolog, Answer Set},
file = {:home/kiyo/Dropbox/Individual Project/book/ASP{\_}ch/07.0{\_}pp{\_}40{\_}60{\_}Roots{\_}of{\_}Answer{\_}Set{\_}Prolog.pdf:pdf},
isbn = {9781139342124},
number = {May},
pages = {40--60},
title = {{Roots of Answer Set Prolog}},
year = {2018}
}
@article{Gebser2011,
abstract = {This paper gives an overview of the open source project Potassco, the Potsdam Answer Set Solving Collection, bundling tools for Answer Set Programming developed at the University of Potsdam.},
author = {Gebser, Martin and Kaufmann, Benjamin and Kaminski, Roland and Ostrowski, Max and Schaub, Torsten and Schneider, Marius},
doi = {10.3233/AIC-2011-0491},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gebser et al. - 2011 - Potassco The Potsdam answer set solving collection.pdf:pdf},
issn = {09217126},
journal = {AI Communications},
keywords = {Answer set programming,declarative problem solving},
number = {2},
pages = {107--124},
title = {{Potassco: The Potsdam answer set solving collection}},
volume = {24},
year = {2011}
}
@article{Corapi2012,
author = {Corapi, Domenico and Russo, Alessandra and Lupu, Emil},
doi = {10.1007/978-3-642-31951-8_12},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Corapi, Russo, Lupu - 2012 - Inductive Logic Programming in Answer Set Programming.pdf:pdf},
isbn = {9783642319501},
issn = {03029743},
journal = {Inductive Logic Programming},
pages = {91--97},
title = {{Inductive Logic Programming in Answer Set Programming}},
year = {2012}
}
@article{Inoue2014,
author = {Inoue, Katsumi and Ribeiro, Tony and Sakama, Chiaki},
doi = {10.1007/s10994-013-5353-8},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Inoue, Ribeiro, Sakama - 2014 - Learning from interpretation transition.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Attractors,Boolean networks,Cellular automata,Dynamical systems,Inductive logic programming,Learning from interpretation,Supported models},
number = {1},
pages = {51--79},
title = {{Learning from interpretation transition}},
volume = {94},
year = {2014}
}
@article{Otero2001,
author = {Otero, Ramon P},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Otero - 2001 - Induction of Stable Models.pdf:pdf},
isbn = {3540425381},
issn = {03029743},
journal = {Conference on Inductive Logic Programming (ILP)},
pages = {193--205},
title = {{Induction of Stable Models}},
year = {2001}
}
@article{DeRaedt1997,
abstract = {Three different formalizations of concept-learning in logic (as well as some variants) are analyzed and related. It is shown that learning from interpretations reduces to learning from entailment, which in turn reduces to learning from satisfiability. The implications of this result for inductive logic programming and computational learning theory are then discussed, and guidelines for choosing a problem-setting are formulated. {\textcopyright} 1997 Elsevier Science B.V.},
author = {{De Raedt}, Luc},
doi = {10.1016/S0004-3702(97)00041-6},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/De Raedt - 1997 - Logical settings for concept-learning.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Computational learning theory,Concept-learning,Inductive logic programming,Logic},
number = {1},
pages = {187--201},
title = {{Logical settings for concept-learning}},
volume = {95},
year = {1997}
}
@article{Sutton1990,
abstract = {This paper extends previous work with Dyna, a class of architectures for intelligent systems based on approximating dynamic program-ming methods. Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned model of the world. In this paper, I present and show results for two D y n a a r c hi-tectures. The Dyna-PI architecture is based on dynamic programming's policy iteration method and can be related to existing AI ideas such a s e v aluation functions and uni-versal plans (reactive systems). Using a nav-igation task, results are shown for a simple Dyna-PI system that simultaneously learns by trial and error, learns a world model, and plans optimal routes using the evolving world model. The Dyna-Q architecture is based on Watkins's Q-learning, a new kind of rein-forcement learning. Dyna-Q uses a less famil-iar set of data structures than does Dyna-PI, but is arguably simpler to implement and use. We s h o w that Dyna-Q architectures are easy to adapt for use in changing environments.},
author = {Sutton, Richard S.},
doi = {10.1016/B978-1-55860-141-3.50030-4},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton - 1990 - Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming.pdf:pdf},
isbn = {1558601414},
issn = {14726963},
journal = {Machine Learning Proceedings 1990},
number = {1987},
pages = {216--224},
pmid = {21542939},
title = {{Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B9781558601413500304},
volume = {02254},
year = {1990}
}
@article{Montague1999,
abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Montague, P.Read},
doi = {10.1016/S1364-6613(99)01331-5},
eprint = {1603.02199},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Montague - 1999 - Reinforcement Learning An Introduction, by Sutton, R.S. and Barto, A.G.pdf:pdf},
isbn = {0262193981},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {9},
pages = {360},
pmid = {18255791},
title = {{Reinforcement Learning: An Introduction, by Sutton, R.S. and Barto, A.G.}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661399013315},
volume = {3},
year = {1999}
}
@article{Levine2015,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a partially observed guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
archivePrefix = {arXiv},
arxivId = {1504.00702},
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1504.00702},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Levine et al. - 2015 - End-to-End Training of Deep Visuomotor Policies.pdf:pdf},
isbn = {9781479969227},
issn = {15337928},
month = {apr},
pmid = {15003161},
title = {{End-to-End Training of Deep Visuomotor Policies}},
url = {http://arxiv.org/abs/1504.00702},
year = {2015}
}
@article{Gelfond1988,
abstract = {We propose a new declarative semantics for logic programs with negation. Its formulation is quite simple; at the same time, it is more general than the iterated fixed point semantics for stratied programs, and is applicable to some useful programs that are not stratified.},
author = {Gelfond, Michael and Lifschitz, Vladimir},
doi = {10.1.1.24.6050},
isbn = {0262610566},
issn = {00224812},
journal = {5th International Conf. of Symp. on Logic Programming},
number = {December 2014},
pages = {1070--1080},
title = {{The stable model semantics for logic programming}},
year = {1988}
}
@article{Gelfond1988a,
abstract = {We propose a new declarative semantics for logic programs with negation. Its formulation is quite simple; at the same time, it is more general than the iterated fixed point semantics for stratied programs, and is applicable to some useful programs that are not stratified.},
author = {Gelfond, Michael and Lifschitz, Vladimir},
doi = {10.1.1.24.6050},
isbn = {0262610566},
issn = {00224812},
journal = {5th International Conf. of Symp. on Logic Programming},
number = {December 2014},
pages = {1070--1080},
title = {{The stable model semantics for logic programming}},
year = {1988}
}
@unknown{unknown,
author = {Schaul, Tom},
isbn = {978-1-4673-5308-3},
pages = {1--8},
title = {{A video game description language for model-based or interactive learning}},
year = {2013}
}
@article{Nielsen2015,
abstract = {—We describe an attempt to generate complete arcade games using the Video Game Description Language (VGDL) and the General Video Game Playing environment (GVG-AI). Games are generated by an evolutionary algorithm working on genotypes represented as VGDL descriptions. In order to direct evolution towards good games, we need an evaluation function that accurately estimates game quality. The evaluation function used here is based on the differential performance of several game-playing algorithms, or Relative Algorithm Performance Profiles (RAPP): it is assumed that good games allow good players to play better than bad players. For the purpose of such evaluations, we introduce two new game tree search algorithms, DeepSearch and Explorer; these perform very well on benchmark games and constitute a substantial subsidiary contribution of the paper. In the end, the attempt to generate arcade games is only partially successful, as some of the games have interesting design features but are barely playable as generated. An analysis of these shortcomings yields several suggestions to guide future attempts at arcade game generation.},
author = {Nielsen, Thorbjorn S. and Barros, Gabriella A.B. and Togelius, Julian and Nelson, Mark J.},
doi = {10.1109/CIG.2015.7317941},
file = {:home/kiyo/Dropbox/Individual Project/GVGAI/GeneratingVGDL{\_}CIG15.pdf:pdf},
isbn = {9781479986217},
journal = {2015 IEEE Conference on Computational Intelligence and Games, CIG 2015 - Proceedings},
pages = {185--192},
title = {{Towards generating arcade game rules with VGDL}},
year = {2015}
}
@article{Law2014,
abstract = {{\textcopyright} Springer International Publishing Switzerland 2014.Existing work on Inductive Logic Programming (ILP) has focused mainly on the learning of definite programs or normal logic programs. In this paper, we aim to push the computational boundary to a wider class of programs: Answer Set Programs. We propose a new paradigm for ILP that integrates existing notions of brave and cautious semantics within a unifying learning framework whose inductive solutions are Answer Set Programs and examples are partial interpretations We present an algorithm that is sound and complete with respect to our new notion of inductive solutions. We demonstrate its applicability by discussing a prototype implementation, called ILASP (Inductive Learning of Answer Set Programs), and evaluate its use in the context of planning. In particular, we show how ILASP can be used to learn agent's knowledge about the environment. Solutions of the learned ASP program provide plans for the agent to travel through the given environment.},
archivePrefix = {arXiv},
arxivId = {1608.01946},
author = {Law, Mark and Russo, Alessandra and Broda, Krysia},
eprint = {1608.01946},
file = {:home/kiyo/Dropbox/Individual Project/ILP/ILASP{\_}Paper.pdf:pdf},
issn = {16113349},
journal = {European Conference on Logics in Artificial Intelligence (JELIA)},
keywords = {inductive reasoning,learning answer set programs,monotonic inductive logic programming,non-},
number = {Ray 2009},
pages = {311--325},
title = {{Inductive Learning of Answer Set Programs}},
volume = {2},
year = {2014}
}
@article{Law2016,
abstract = {In recent years, several frameworks and systems have been proposed that extend Inductive Logic Programming (ILP) to the Answer Set Programming (ASP) paradigm. In ILP, examples must all be explained by a hypothesis together with a given background knowledge. In existing systems, the background knowledge is the same for all examples; however, examples may be context-dependent. This means that some examples should be explained in the context of some information, whereas others should be explained in different contexts. In this paper, we capture this notion and present a context-dependent extension of the Learning from Ordered Answer Sets framework. In this extension, contexts can be used to further structure the background knowledge. We then propose a new iterative algorithm, ILASP2i, which exploits this feature to scale up the existing ILASP2 system to learning tasks with large numbers of examples. We demonstrate the gain in scalability by applying both algorithms to various learning tasks. Our results show that, compared to ILASP2, the newly proposed ILASP2i system can be two orders of magnitude faster and use two orders of magnitude less memory, whilst preserving the same average accuracy. This paper is under consideration for acceptance in TPLP.},
archivePrefix = {arXiv},
arxivId = {1608.01946},
author = {Law, Mark and Russo, Alessandra and Broda, Krysia},
eprint = {1608.01946},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Law, Russo, Broda - 2016 - Iterative Learning of Answer Set Programs from Context Dependent Examples(2).pdf:pdf},
month = {aug},
title = {{Iterative Learning of Answer Set Programs from Context Dependent Examples}},
url = {http://arxiv.org/abs/1608.01946},
year = {2016}
}
@article{Law2015,
abstract = {This paper contributes to the area of inductive logic programming by presenting a new learning framework that allows the learning of weak constraints in Answer Set Programming (ASP). The framework, called Learning from Ordered Answer Sets, generalises our previous work on learning ASP programs without weak constraints, by considering a new notion of examples as ordered pairs of partial answer sets that exemplify which answer sets of a learned hypothesis (together with a given background knowledge) are preferred to others. In this new learning task inductive solutions are searched within a hypothesis space of normal rules, choice rules, and hard and weak constraints. We propose a new algorithm, ILASP2, which is sound and complete with respect to our new learning framework. We investigate its applicability to learning preferences in an interview scheduling problem and also demonstrate that when restricted to the task of learning ASP programs without weak constraints, ILASP2 can be much more efficient than our previously proposed system.},
archivePrefix = {arXiv},
arxivId = {1507.06566},
author = {Law, Mark and Russo, Alessandra and Broda, Krysia},
doi = {10.1017/S1471068415000198},
eprint = {1507.06566},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Law, Russo, Broda - 2015 - Learning Weak Constraints in Answer Set Programming.pdf:pdf},
month = {jul},
title = {{Learning Weak Constraints in Answer Set Programming}},
url = {http://arxiv.org/abs/1507.06566 http://dx.doi.org/10.1017/S1471068415000198},
year = {2015}
}
@article{Garcez2018,
archivePrefix = {arXiv},
arxivId = {1804.08597},
author = {d'Avila Garcez, Artur and Dutra, Aimore Resende Riquetti and Alonso, Eduardo},
eprint = {1804.08597},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garcez, Dutra, Alonso - 2018 - Towards Symbolic Reinforcement Learning with Common Sense.pdf:pdf},
month = {apr},
title = {{Towards Symbolic Reinforcement Learning with Common Sense}},
url = {http://arxiv.org/abs/1804.08597},
year = {2018}
}
@article{Garnelo2016,
abstract = {Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-of-concept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system -- though just a prototype -- learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game.},
archivePrefix = {arXiv},
arxivId = {1609.05518},
author = {Garnelo, Marta and Arulkumaran, Kai and Shanahan, Murray},
eprint = {1609.05518},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garnelo, Arulkumaran, Shanahan - 2016 - Towards Deep Symbolic Reinforcement Learning.pdf:pdf},
month = {sep},
title = {{Towards Deep Symbolic Reinforcement Learning}},
url = {http://arxiv.org/abs/1609.05518},
year = {2016}
}
@article{Ebner2013,
abstract = {This chapter is a direct follow-up to the chapter on General Video Game Playing (GVGP). As that group recognised the need to create a Video Game Description Language (VGDL), we formed a group to address that challenge and the results of that group is the current chapter. Unlike the VGDL envisioned in the previous chapter, the language envisioned here is not meant to be supplied to the game-playing agent for automatic reasoning; instead we argue that the agent should learn this from interaction with the system. The main purpose of the language proposed here is to be able to specify complete video games, so that they could be compiled with a special VGDL compiler. Implementing such a compiler could provide numerous opportunities; users could modify existing games very quickly, or have a library of existing implementations defined within the language (e.g. an Asteroids ship or a Mario avatar) that have pre-existing, parameterised behaviours that can be customised for the users specific purposes. Provided the language is fit for purpose, automatic game creation could be explored further through experimentation with machine learning algorithms, furthering research in game creation and design. In order for both of these perceived functions to be realised and to ensure it is suitable for a large user base we recognise that the language carries several key requirements. Not only must it be human-readable, but retain the capability to be both expressive and extensible whilst equally simple as it is general. In our preliminary discussions, we sought to define the key requirements and challenges in constructing a new VGDL that will become part of the GVGP process. From this we have proposed an initial design to the semantics of the language and the components required to define a given game. Furthermore, we applied this approach to represent classic games such as Space Invaders, Lunar Lander and Frogger in an attempt to identify potential problems that may come to light. Work is ongoing to realise the potential of the VGDL for the purposes of Procedural Content Generation, Automatic Game Design and Transfer Learning.},
author = {Ebner, Marc and Levine, John and Lucas, SM},
doi = {10.4230/DFU.Vol6.12191.85},
file = {:home/kiyo/Dropbox/Individual Project/GVGAI/Towards a Video Game Description Language.pdf:pdf},
isbn = {9783939897620},
journal = {Dagstuhl Follow- {\ldots}},
pages = {85--100},
title = {{Towards a video game description language}},
url = {http://drops.dagstuhl.de/opus/volltexte/2013/4338/},
volume = {6},
year = {2013}
}
@article{Bianchi2017,
abstract = {Reinforcement Learning (RL) is a well-known technique for learning the solutions of control problems from the interactions of an agent in its domain. However, RL is known to be inefficient in problems of the real- world where the state space and the set of actions grow up fast. Recently, heuristics, case-based reasoning (CBR) and transfer learning have been used as tools to accel- erate the RL process. This paper investigates a class of algorithms called Transfer Learning Heuristically Acceler- ated Reinforcement Learning (TLHARL) that uses CBR as heuristics within a transfer learning setting to accelerate RL. The main contributions of this work are the proposal of a new TLHARL algorithm based on the traditional RL algo- rithm Q($\lambda$) and the application of TLHARL on two distinct real-robot domains: a robot soccer with small-scale robots and the humanoid-robot stability learning. Experimental results show that our proposed method led to a significant improvement of the learning rate in both domains.},
author = {Bianchi, Reinaldo A.C. and Santos, Paulo E. and da Silva, Isaac J. and Celiberto, Luiz A. and {Lopez de Mantaras}, Ramon},
doi = {10.1007/s10846-017-0731-2},
file = {:home/kiyo/Dropbox/Individual Project/meta{\_}learning/Heuristically Accelerated Reinforcement Learning.pdf:pdf},
issn = {15730409},
journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
keywords = {Case-based reasoning,Reinforcement learning,Robotics,Transfer learning},
pages = {1--12},
publisher = {Journal of Intelligent {\&} Robotic Systems},
title = {{Heuristically Accelerated Reinforcement Learning by Means of Case-Based Reasoning and Transfer Learning}},
year = {2017}
}
@article{Savarese2016,
abstract = {We propose a new layer design by adding a linear gating mechanism to shortcut connections. By using a scalar parameter to control each gate, we provide a way to learn identity mappings by optimizing only one parameter. We build upon the motivation behind Residual Networks, where a layer is reformulated in order to make learning identity mappings less problematic to the optimizer. The augmentation introduces only one extra parameter per layer, and provides easier optimization by making degeneration into identity mappings simpler. We propose a new model, the Gated Residual Network, which is the result when augmenting Residual Networks. Experimental results show that augmenting layers provides better optimization, increased performance, and more layer independence. We evaluate our method on MNIST using fully-connected networks, showing empirical indications that our augmentation facilitates the optimization of deep models, and that it provides high tolerance to full layer removal: the model retains over 90{\%} of its performance even after half of its layers have been randomly removed. We also evaluate our model on CIFAR-10 and CIFAR-100 using Wide Gated ResNets, achieving 3.65{\%} and 18.27{\%} error, respectively.},
archivePrefix = {arXiv},
arxivId = {1611.01260},
author = {Savarese, Pedro H. P. and Mazza, Leonardo O. and Figueiredo, Daniel R.},
eprint = {1611.01260},
file = {:home/kiyo/Dropbox/Individual Project/meta{\_}learning/LEARNING INVARIANCES FOR POLICY.pdf:pdf},
isbn = {0897914597},
journal = {Analysis},
number = {2},
pages = {245--278},
title = {{Learning Identity Mappings with Residual Gates}},
url = {http://arxiv.org/abs/1611.01260},
volume = {29},
year = {2016}
}
@article{Taylor2009,
abstract = {The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.},
archivePrefix = {arXiv},
arxivId = {0803.0476},
author = {Taylor, Matthew E and Stone, Peter},
doi = {10.1007/978-3-642-27645-3},
eprint = {0803.0476},
file = {:home/kiyo/Dropbox/Individual Project/meta{\_}learning/Transfer Learning for Reinforcement Learning Domains$\backslash$: A Survey.pdf:pdf},
isbn = {15324435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {1,1998,actions with goal,example,leaning agents take sequential,maximizing a reward,multi task learning,problems,reinforcement learning,rl,signal,sutton barto,transfer learning,transfer learning objectives,which may time delayed},
pages = {1633--1685},
pmid = {260529900010},
title = {{Transfer Learning for Reinforcement Learning Domains : A Survey}},
url = {http://portal.acm.org/citation.cfm?id=1755839},
volume = {10},
year = {2009}
}
@article{Perez-Liebana2017,
abstract = {—This paper presents a study on the robustness and variability of performance of general video game-playing agents. Agents analyzed includes those that won the different legs of the 2014 and 2015 General Video Game AI Competitions, and two sample agents distributed with its framework. Initially, these agents are run in four games and ranked according to the rules of the competition. Then, different modifications to the reward signal of the games are proposed and noise is introduced in either the actions executed by the controller, their forward model, or both. Results show that it is possible to produce a significant change in the rankings by introducing the modifications proposed here. This is an important result because it enables the set of human-authored games to be automatically expanded by adding parameter-varied versions that add information and insight into the relative strengths of the agents under test. Results also show that some controllers perform well under almost all conditions, a testament to the robustness of the GVGAI benchmark.},
author = {Perez-Liebana, Diego and Samothrakis, Spyridon and Togelius, Julian and Schaul, Tom and Lucas, Simon M.},
doi = {10.1109/CIG.2016.7860430},
file = {:home/kiyo/Dropbox/Individual Project/GVGAI/Analyzing the Robustness of FVGPA.pdf:pdf},
isbn = {9781509018833},
issn = {23254289},
journal = {IEEE Conference on Computatonal Intelligence and Games, CIG},
title = {{Analyzing the robustness of general video game playing agents}},
year = {2017}
}
@article{Neufeld2015,
abstract = {—This paper proposes an automatic way of evolving level generators for arbitrary 2D games, which are described in the Video Game Description Language (VGDL). The process works as follows: a game described in VGDL is interpreted and transformed in a set of rules defined in Answer Set Programming (ASP), along with other general and customizable rules. Although a set of rules described in ASP can generate multiple levels, not all of them will be playable or well designed. Therefore, an evolutionary process is run to determine the values of the parameters of those customizable rules. The different level generators are evaluated with general video game playing agents, which are able to play any game and level in the framework. The aim is to maximize the difference between their performance in the levels generated, under the assumption that levels are better designed if good skilled players play better than poor agents. This work presents some initial experiments that suggest that it is possible to evolve interesting level generators using this technique, and outlines some lines of future work.},
author = {Neufeld, Xenija and Mostaghim, Sanaz and Perez-Liebana, Diego},
doi = {10.1109/CEEC.2015.7332726},
file = {:home/kiyo/Dropbox/Individual Project/GVGAI/Procedural Level Generation with Answer Set.pdf:pdf},
isbn = {9781467394819},
journal = {2015 7th Computer Science and Electronic Engineering Conference, CEEC 2015 - Conference Proceedings},
pages = {207--212},
title = {{Procedural level generation with answer set programming for general Video Game playing}},
year = {2015}
}
@article{Wang2018,
abstract = {Despite the recent successes of deep neural networks in various fields such as image and speech recognition, natural language processing, and reinforcement learning, we still face big challenges in bringing the power of numeric optimization to symbolic reasoning. Researchers have proposed different avenues such as neural machine translation for proof synthesis, vectorization of symbols and expressions for representing symbolic patterns, and coupling of neural back-ends for dimensionality reduction with symbolic front-ends for decision making. However, these initial explorations are still only point solutions, and bear other shortcomings such as lack of correctness guarantees. In this paper, we present our approach of casting symbolic reasoning as games, and directly harnessing the power of deep reinforcement learning in the style of Alpha(Go) Zero on symbolic problems. Using the Boolean Satisfiability (SAT) problem as showcase, we demonstrate the feasibility of our method, and the advantages of modularity, efficiency, and correctness guarantees.},
archivePrefix = {arXiv},
arxivId = {1802.05340},
author = {Wang, Fei and Rompf, Tiark},
eprint = {1802.05340},
file = {:home/kiyo/Dropbox/Individual Project/symbolic{\_}approach/FROM GAMEPLAY TO SYMBOLIC REASONING- LEARNING SAT SOLVER HEURISTICS IN THE STYLE OF ALPHA(GO) ZERO.pdf:pdf},
pages = {1--4},
title = {{From Gameplay to Symbolic Reasoning: Learning SAT Solver Heuristics in the Style of Alpha(Go) Zero}},
url = {http://arxiv.org/abs/1802.05340},
year = {2018}
}
@article{Apeldoorn2017,
author = {Apeldoorn, Daan and Gabriele, Kern-Isberner},
file = {:home/kiyo/Dropbox/Individual Project/PaperOnReinforcementLearning.pdf:pdf},
journal = {Proceedings of the Thirteenth International Symposium on Commonsense Reasoning},
pages = {1--8},
title = {{An Agent-Based Learning Approach for Finding and Exploiting Heuristics in Unknown Environments}},
year = {2017}
}
@article{Russo,
author = {Russo, Alessandra and Law, Mark and Broda, Krysia},
file = {:home/kiyo/Dropbox/Individual Project/ILP/IL of Human Behaviours.pdf:pdf},
pages = {1--3},
title = {{Inductive Learning of Human Behaviours}}
}
@article{Dragiev2017,
abstract = {The integration of abduction and induction has lead to a variety of non-monotonic ILP systems. XHAIL is one of these systems, in which abduction is used to compute hypotheses that subsume Kernel Sets. On the other hand, Peircebayes is a recently proposed logic-based probabilistic programming approach that combines abduction with parameter learning to learn distributions of most likely explanations. In this paper, we propose an approach for integrating probabilistic inference with ILP. The basic idea is to redefine the inductive task of XHAIL as a statistical abduction, and to use Peircebayes to learn probability distribution of hypotheses. An initial evaluation of the proposed algorithm is given using synthetic data.},
author = {Dragiev, Stanislav and Russo, Alessandra and Broda, Krysia and Law, Mark and Turliuc, Rares},
file = {:home/kiyo/Dropbox/Individual Project/ILP/An Abductive-Inductive Algorithm for Probabilistic Inductive Logic Programming.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
pages = {20--26},
title = {{An abductive-inductive algorithm for probabilistic inductive logic programming}},
volume = {1865},
year = {2017}
}
@article{Muggleton1999,
author = {Muggleton, S H},
file = {:home/kiyo/Dropbox/Individual Project/ILP/ILP$\backslash$: Challenges:},
journal = {The MIT Encyclopedia of the Cognitive Sciences (MITECS)},
keywords = {What's Hot Abstracts},
pages = {4330--4332},
title = {{Inductive {\{}L{\}}ogic {\{}P{\}}rogramming}},
year = {1999}
}
@article{Arulkumaran2017,
abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep {\$}Q{\$}-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
archivePrefix = {arXiv},
arxivId = {1708.05866},
author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
doi = {10.1109/MSP.2017.2743240},
eprint = {1708.05866},
file = {:home/kiyo/Dropbox/Individual Project/DRL/a{\_}brief{\_}survey{\_}of{\_}DRL.pdf:pdf},
isbn = {9781424469178},
issn = {1701.07274},
pages = {1--16},
pmid = {25719670},
title = {{A Brief Survey of Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1708.05866{\%}0Ahttp://dx.doi.org/10.1109/MSP.2017.2743240},
year = {2017}
}
@article{Yang2017,
abstract = {Modern machine learning methods are increasingly powerful and opaque. This opaqueness is a concern across a variety of domains in which algorithms are making important decisions that should be scrutable. The explainabilty of machine learning systems is therefore of increasing interest. We propose an explanation-by-examples approach that builds on our recent research in Bayesian teaching in which we aim to select a small subset of the data that would lead the learner to similar conclusions as the entire dataset. We discuss this approach, explicating several key advantages. First, the ability to cover any model with a probabilistic interpretation including supervised, unsupervised, and reinforcement learning (including deep learning). Second, we discuss the empirical foundations of this approach in the cognitive science of learning from other agents. Third, we outline challenges to full realization of the promise of this approach. We conclude by discussing implications for machine learning and applications to real-world problems.},
author = {Yang, Scott Chen-Hsin and Shafto, Patrick},
file = {:home/kiyo/Dropbox/Individual Project/meta{\_}learning/Explainable Artificial Intelligence via Bayesian.pdf:pdf},
journal = {Conference on Neural Information Processing Systems},
number = {Nips},
title = {{Explainable Artificial Intelligence via Bayesian Teaching}},
year = {2017}
}
@article{Felix2016,
abstract = {{\textcopyright} Springer International Publishing Switzerland 2016. Machine learning processes consist in collecting data, obtaining a model and applying it to a given task. Given a new task, the standard approach is to restart the learning process and obtain a new model. However, previous learning experience can be exploited to assist the new learning process. The two most studied approaches for this are metalearning and transfer learning. Metalearning can be used for selecting the predictive model to use on a new dataset. Transfer learning allows the reuse of knowledge from previous tasks. However, when multiple heterogeneous tasks are available as potential sources for transfer, the question is which one to use. One approach to address this problem is metalearning. In this paper we investigate the feasibility of this approach. We propose a method to transfer weights from a source trained neural network to initialize a network that models a potentially very different target dataset. Our experiments with 14 datasets indicate that this method enables faster convergence without significant difference in accuracy provided that the source task is adequately chosen. This means that there is potential for applying metalearning to support transfer between heterogeneous datasets.},
author = {F{\'{e}}lix, Catarina and Soares, Carlos and Jorge, Al{\'{i}}pio},
doi = {10.1007/978-3-319-32034-2_28},
file = {:home/kiyo/Dropbox/Individual Project/meta{\_}learning/Can metalearning be applied to transfer on.pdf:pdf},
isbn = {9783319320335},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {332--343},
title = {{Can metalearning be applied to transfer on heterogeneous datasets?}},
volume = {9648},
year = {2016}
}
@article{Desjardins-Proulx2017,
abstract = {Artificial Intelligence presents an important paradigm shift for science. Science is traditionally founded on theories and models, most often formalized with mathematical formulas handcrafted by theoretical scientists and refined through experiments. Machine learning, an important branch of modern Artificial Intelligence, focuses on learning from data. This leads to a fundamentally different approach to model-building: we step back and focus on the design of algorithms capable of building models from data, but the models themselves are not designed by humans. This is even more true with deep learning, which requires little engineering by hand and is responsible for many of Artificial Intelligence's spectacular successes. In contrast to logic systems, knowledge from a deep learning model is difficult to understand, reuse, and may involve up to a billion parameters. On the other hand, probabilistic machine learning techniques such as deep learning offer an opportunity to tackle large complex problems that are out of the reach of traditional theory-making. It is possible that the more intuition-like reasoning performed by deep learning systems is mostly incompatible with the logic formalism of mathematics. Yet recent studies have shown that deep learning can be useful to logic systems and vice versa. Success at unifying different paradigms of Artificial Intelligence from logic to probability theory offers unique opportunities to combine data-driven approaches with traditional theories. These advancements are susceptible to impact significantly biological sciences, where dimensionality is high and limit the investigation of traditional theories.},
author = {Desjardins-Proulx, Philippe and Poisot, Timoth{\'{e}}e and Gravel, Dominique},
doi = {10.1101/161125},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Desjardins-Proulx, Poisot, Gravel - 2017 - Scientific Theories and Artificial Intelligence.pdf:pdf},
journal = {bioRxiv},
month = {oct},
pages = {161125},
publisher = {Cold Spring Harbor Laboratory},
title = {{Scientific Theories and Artificial Intelligence}},
url = {https://www.biorxiv.org/content/early/2017/10/22/161125},
year = {2017}
}
@article{Lake2016,
abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
archivePrefix = {arXiv},
arxivId = {1604.00289},
author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
eprint = {1604.00289},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lake et al. - 2016 - Building Machines That Learn and Think Like People.pdf:pdf},
month = {apr},
title = {{Building Machines That Learn and Think Like People}},
url = {http://arxiv.org/abs/1604.00289},
year = {2016}
}
@article{Penkov2017,
archivePrefix = {arXiv},
arxivId = {1705.08320},
author = {Penkov, Svetlin and Ramamoorthy, Subramanian},
eprint = {1705.08320},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Penkov, Ramamoorthy - 2017 - Explaining Transition Systems through Program Induction.pdf:pdf},
month = {may},
title = {{Explaining Transition Systems through Program Induction}},
url = {http://arxiv.org/abs/1705.08320},
year = {2017}
}
@article{Ferreira2017,
abstract = {Non-stationary domains, where unforeseen changes happen, present a challenge for agents to find an optimal policy for a sequential decision making problem. This work investigates a solution to this problem that combines Markov Decision Processes (MDP) and Reinforcement Learning (RL) with Answer Set Programming (ASP) in a method we call ASP(RL). In this method, Answer Set Programming is used to find the possible trajectories of an MDP, from where Reinforcement Learning is applied to learn the optimal policy of the problem. Results show that ASP(RL) is capable of efficiently finding the optimal solution of an MDP representing non-stationary domains.},
archivePrefix = {arXiv},
arxivId = {1705.01399},
author = {Ferreira, Leonardo A. and Bianchi, Reinaldo A. C. and Santos, Paulo E. and de Mantaras, Ramon Lopez},
eprint = {1705.01399},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ferreira et al. - 2017 - Answer Set Programming for Non-Stationary Markov Decision Processes.pdf:pdf},
month = {may},
title = {{Answer Set Programming for Non-Stationary Markov Decision Processes}},
url = {http://arxiv.org/abs/1705.01399},
year = {2017}
}
@article{Ferreira2017a,
abstract = {Non-stationary domains, that change in unpredicted ways, are a challenge for agents searching for optimal policies in sequential decision-making problems. This paper presents a combination of Markov Decision Processes (MDP) with Answer Set Programming (ASP), named {\{}$\backslash$em Online ASP for MDP{\}} (oASP(MDP)), which is a method capable of constructing the set of domain states while the agent interacts with a changing environment. oASP(MDP) updates previously obtained policies, learnt by means of Reinforcement Learning (RL), using rules that represent the domain changes observed by the agent. These rules represent a set of domain constraints that are processed as ASP programs reducing the search space. Results show that oASP(MDP) is capable of finding solutions for problems in non-stationary domains without interfering with the action-value function approximation process.},
archivePrefix = {arXiv},
arxivId = {1706.01417},
author = {Ferreira, Leonardo A. and Bianchi, Reinaldo A. C. and Santos, Paulo E. and de Mantaras, Ramon Lopez},
eprint = {1706.01417},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ferreira et al. - 2017 - A method for the online construction of the set of states of a Markov Decision Process using Answer Set Program.pdf:pdf},
month = {jun},
title = {{A method for the online construction of the set of states of a Markov Decision Process using Answer Set Programming}},
url = {http://arxiv.org/abs/1706.01417},
year = {2017}
}
@article{Cai2017,
abstract = {This paper introduces an SLD-resolution technique based on deep learning. This technique enables neural networks to learn from old and successful resolution processes and to use learnt experiences to guide new resolution processes. An implementation of this technique is named SLDR-DL. It includes a Prolog library of deep feedforward neural networks and some essential functions of resolution. In the SLDR-DL framework, users can define logical rules in the form of definite clauses and teach neural networks to use the rules in reasoning processes.},
archivePrefix = {arXiv},
arxivId = {1705.02210},
author = {Cai, Cheng-Hao},
eprint = {1705.02210},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cai - 2017 - SLDR-DL A Framework for SLD-Resolution with Deep Learning.pdf:pdf},
month = {may},
title = {{SLDR-DL: A Framework for SLD-Resolution with Deep Learning}},
url = {http://arxiv.org/abs/1705.02210},
year = {2017}
}
@article{Penkov2017a,
abstract = {Explaining and reasoning about processes which underlie observed black-box phenomena enables the discovery of causal mechanisms, derivation of suitable abstract representations and the formulation of more robust predictions. We propose to learn high level functional programs in order to represent abstract models which capture the invariant structure in the observed data. We introduce the {\$}\backslashpi{\$}-machine (program-induction machine) -- an architecture able to induce interpretable LISP-like programs from observed data traces. We propose an optimisation procedure for program learning based on backpropagation, gradient descent and A* search. We apply the proposed method to two problems: system identification of dynamical systems and explaining the behaviour of a DQN agent. Our results show that the {\$}\backslashpi{\$}-machine can efficiently induce interpretable programs from individual data traces.},
archivePrefix = {arXiv},
arxivId = {1708.00376},
author = {Penkov, Svetlin and Ramamoorthy, Subramanian},
eprint = {1708.00376},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Penkov, Ramamoorthy - 2017 - Using Program Induction to Interpret Transition System Dynamics.pdf:pdf},
month = {jul},
title = {{Using Program Induction to Interpret Transition System Dynamics}},
url = {http://arxiv.org/abs/1708.00376},
year = {2017}
}
@article{Verma2018,
abstract = {We study the problem of generating interpretable and verifiable policies through reinforcement learning. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, in which the policy is represented by a neural network, the aim in Programmatically Interpretable Reinforcement Learning is to find a policy that can be represented in a high-level programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maxima reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural "oracle". We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also find that a well-designed policy language can serve as a regularizer, and result in the discovery of policies that lead to smoother trajectories and are more easily transferred to environments not encountered during training.},
archivePrefix = {arXiv},
arxivId = {1804.02477},
author = {Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
eprint = {1804.02477},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Verma et al. - 2018 - Programmatically Interpretable Reinforcement Learning.pdf:pdf},
month = {apr},
title = {{Programmatically Interpretable Reinforcement Learning}},
url = {http://arxiv.org/abs/1804.02477},
year = {2018}
}
@article{Law2016a,
abstract = {In recent years, several frameworks and systems have been proposed that extend Inductive Logic Programming (ILP) to the Answer Set Programming (ASP) paradigm. In ILP, examples must all be explained by a hypothesis together with a given background knowledge. In existing systems, the background knowledge is the same for all examples; however, examples may be context-dependent. This means that some examples should be explained in the context of some information, whereas others should be explained in different contexts. In this paper, we capture this notion and present a context-dependent extension of the Learning from Ordered Answer Sets framework. In this extension, contexts can be used to further structure the background knowledge. We then propose a new iterative algorithm, ILASP2i, which exploits this feature to scale up the existing ILASP2 system to learning tasks with large numbers of examples. We demonstrate the gain in scalability by applying both algorithms to various learning tasks. Our results show that, compared to ILASP2, the newly proposed ILASP2i system can be two orders of magnitude faster and use two orders of magnitude less memory, whilst preserving the same average accuracy. This paper is under consideration for acceptance in TPLP.},
archivePrefix = {arXiv},
arxivId = {1608.01946},
author = {Law, Mark and Russo, Alessandra and Broda, Krysia},
eprint = {1608.01946},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Law, Russo, Broda - 2016 - Iterative Learning of Answer Set Programs from Context Dependent Examples(2).pdf:pdf},
month = {aug},
title = {{Iterative Learning of Answer Set Programs from Context Dependent Examples}},
url = {http://arxiv.org/abs/1608.01946},
year = {2016}
}
@article{Zhou2018,
abstract = {Few-shot learning remains challenging for meta-learning that learns a learning algorithm (meta-learner) from many related tasks. In this work, we argue that this is due to the lack of a good representation for meta-learning, and propose deep meta-learning to integrate the representation power of deep learning into meta-learning. The framework is composed of three modules, a concept generator, a meta-learner, and a concept discriminator, which are learned jointly. The concept generator, e.g. a deep residual net, extracts a representation for each instance that captures its high-level concept, on which the meta-learner performs few-shot learning, and the concept discriminator recognizes the concepts. By learning to learn in the concept space rather than in the complicated instance space, deep meta-learning can substantially improve vanilla meta-learning, which is demonstrated on various few-shot image recognition problems. For example, on 5-way-1-shot image recognition on CIFAR-100 and CUB-200, it improves Matching Nets from 50.53{\%} and 56.53{\%} to 58.18{\%} and 63.47{\%}, improves MAML from 49.28{\%} and 50.45{\%} to 56.65{\%} and 64.63{\%}, and improves Meta-SGD from 53.83{\%} and 53.34{\%} to 61.62{\%} and 66.95{\%}, respectively.},
archivePrefix = {arXiv},
arxivId = {1802.03596},
author = {Zhou, Fengwei and Wu, Bin and Li, Zhenguo},
eprint = {1802.03596},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou, Wu, Li - 2018 - Deep Meta-Learning Learning to Learn in the Concept Space.pdf:pdf},
month = {feb},
title = {{Deep Meta-Learning: Learning to Learn in the Concept Space}},
url = {http://arxiv.org/abs/1802.03596},
year = {2018}
}
@article{Franceschi2017,
abstract = {We consider a class of a nested optimization problems involving inner and outer objectives. We observe that by taking into explicit account the optimization dynamics for the inner objective it is possible to derive a general framework that unifies gradient-based hyperparameter optimization and meta-learning (or learning-to-learn). Depending on the specific setting, the variables of the outer objective take either the meaning of hyperparameters in a supervised learning problem or parameters of a meta-learner. We show that some recently proposed methods in the latter setting can be instantiated in our framework and tackled with the same gradient-based algorithms. Finally, we discuss possible design patterns for learning-to-learn and present encouraging preliminary experiments for few-shot learning.},
archivePrefix = {arXiv},
arxivId = {1712.06283},
author = {Franceschi, Luca and Donini, Michele and Frasconi, Paolo and Pontil, Massimiliano},
eprint = {1712.06283},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Franceschi et al. - 2017 - A Bridge Between Hyperparameter Optimization and Larning-to-learn.pdf:pdf},
month = {dec},
title = {{A Bridge Between Hyperparameter Optimization and Larning-to-learn}},
url = {http://arxiv.org/abs/1712.06283},
year = {2017}
}
@article{Sung2017,
abstract = {We propose a novel and flexible approach to meta-learning for learning-to-learn from only a few examples. Our framework is motivated by actor-critic reinforcement learning, but can be applied to both reinforcement and supervised learning. The key idea is to learn a meta-critic: an action-value function neural network that learns to criticise any actor trying to solve any specified task. For supervised learning, this corresponds to the novel idea of a trainable task-parametrised loss generator. This meta-critic approach provides a route to knowledge transfer that can flexibly deal with few-shot and semi-supervised conditions for both reinforcement and supervised learning. Promising results are shown on both reinforcement and supervised learning problems.},
archivePrefix = {arXiv},
arxivId = {1706.09529},
author = {Sung, Flood and Zhang, Li and Xiang, Tao and Hospedales, Timothy and Yang, Yongxin},
eprint = {1706.09529},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sung et al. - 2017 - Learning to Learn Meta-Critic Networks for Sample Efficient Learning.pdf:pdf},
month = {jun},
title = {{Learning to Learn: Meta-Critic Networks for Sample Efficient Learning}},
url = {http://arxiv.org/abs/1706.09529},
year = {2017}
}
@article{Duan2016,
abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL{\$}{\^{}}2{\$}, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL{\$}{\^{}}2{\$} experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL{\$}{\^{}}2{\$} is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL{\$}{\^{}}2{\$} on a vision-based navigation task and show that it scales up to high-dimensional problems.},
archivePrefix = {arXiv},
arxivId = {1611.02779},
author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
eprint = {1611.02779},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Duan et al. - 2016 - RL{\$}2{\$} Fast Reinforcement Learning via Slow Reinforcement Learning.pdf:pdf},
month = {nov},
title = {{RL{\$}{\^{}}2{\$}: Fast Reinforcement Learning via Slow Reinforcement Learning}},
url = {http://arxiv.org/abs/1611.02779},
year = {2016}
}
@article{Mishra2017,
abstract = {Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information. In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks. On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins.},
archivePrefix = {arXiv},
arxivId = {1707.03141},
author = {Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
eprint = {1707.03141},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mishra et al. - 2017 - A Simple Neural Attentive Meta-Learner.pdf:pdf},
month = {jul},
title = {{A Simple Neural Attentive Meta-Learner}},
url = {http://arxiv.org/abs/1707.03141},
year = {2017}
}
@article{Lesort2018,
abstract = {Representation learning algorithms are designed to learn abstract features that characterize data. State representation learning (SRL) focuses on a particular kind of representation learning where learned features are in low dimension, evolve through time, and are influenced by actions of an agent. As the representation learned captures the variation in the environment generated by agents, this kind of representation is particularly suitable for robotics and control scenarios. In particular, the low dimension helps to overcome the curse of dimensionality, provides easier interpretation and utilization by humans and can help improve performance and speed in policy learning algorithms such as reinforcement learning. This survey aims at covering the state-of-the-art on state representation learning in the most recent years. It reviews different SRL methods that involve interaction with the environment, their implementations and their applications in robotics control tasks (simulated or real). In particular, it highlights how generic learning objectives are differently exploited in the reviewed algorithms. Finally, it discusses evaluation methods to assess the representation learned and summarizes current and future lines of research.},
archivePrefix = {arXiv},
arxivId = {1802.04181},
author = {Lesort, Timoth{\'{e}}e and D{\'{i}}az-Rodr{\'{i}}guez, Natalia and Goudou, Jean-Fran{\c{c}}ois and Filliat, David},
eprint = {1802.04181},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lesort et al. - 2018 - State Representation Learning for Control An Overview.pdf:pdf},
month = {feb},
title = {{State Representation Learning for Control: An Overview}},
url = {http://arxiv.org/abs/1802.04181},
year = {2018}
}
@misc{Stone,
author = {Stone, Josiah Hanna and Peter},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stone - Unknown - Towards a Data Efficient Off-Policy Policy Gradient.pdf:pdf},
title = {{Towards a Data Efficient Off-Policy Policy Gradient}},
url = {http://www.cs.utexas.edu/users/ai-lab/?AAAISSS2018-Hanna{\#}.Ww6m{\_}aBQDCU.mendeley}
}
@article{Perez-Liebana2018,
abstract = {General Video Game Playing (GVGP) aims at designing an agent that is capable of playing multiple video games with no human intervention. In 2014, The General Video Game AI (GVGAI) competition framework was created and released with the purpose of providing researchers a common open-source and easy to use platform for testing their AI methods with potentially infinity of games created using Video Game Description Language (VGDL). The framework has been expanded into several tracks during the last few years to meet the demand of different research directions. The agents are required to either play multiples unknown games with or without access to game simulations, or to design new game levels or rules. This survey paper presents the VGDL, the GVGAI framework, existing tracks, and reviews the wide use of GVGAI framework in research, education and competitions five years after its birth. A future plan of framework improvements is also described.},
archivePrefix = {arXiv},
arxivId = {1802.10363},
author = {Perez-Liebana, Diego and Liu, Jialin and Khalifa, Ahmed and Gaina, Raluca D. and Togelius, Julian and Lucas, Simon M.},
eprint = {1802.10363},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perez-Liebana et al. - 2018 - General Video Game AI a Multi-Track Framework for Evaluating Agents, Games and Content Generation Algorith.pdf:pdf},
month = {feb},
title = {{General Video Game AI: a Multi-Track Framework for Evaluating Agents, Games and Content Generation Algorithms}},
url = {http://arxiv.org/abs/1802.10363},
year = {2018}
}
@article{Woof2018,
abstract = {Deep reinforcement learning (DRL) has proven to be an effective tool for creating general video-game AI. However most current DRL video-game agents learn end-to-end from the video-output of the game, which is superfluous for many applications and creates a number of additional problems. More importantly, directly working on pixel-based raw video data is substantially distinct from what a human player does.In this paper, we present a novel method which enables DRL agents to learn directly from object information. This is obtained via use of an object embedding network (OEN) that compresses a set of object feature vectors of different lengths into a single fixed-length unified feature vector representing the current game-state and fulfills the DRL simultaneously. We evaluate our OEN-based DRL agent by comparing to several state-of-the-art approaches on a selection of games from the GVG-AI Competition. Experimental results suggest that our object-based DRL agent yields performance comparable to that of those approaches used in our comparative study.},
archivePrefix = {arXiv},
arxivId = {1803.05262},
author = {Woof, William and Chen, Ke},
eprint = {1803.05262},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Woof, Chen - 2018 - Learning to Play General Video-Games via an Object Embedding Network.pdf:pdf},
month = {mar},
title = {{Learning to Play General Video-Games via an Object Embedding Network}},
url = {http://arxiv.org/abs/1803.05262},
year = {2018}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
archivePrefix = {arXiv},
arxivId = {1610.00633},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
eprint = {1610.00633},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silver et al. - 2016 - Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
eprint = {1312.5602},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
pmid = {25719670},
title = {{Human-level control through deep reinforcement learning}},
year = {2015}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Mnih2015a,
abstract = {Policy advice is a transfer learning method where a student agent is able to learn faster via advice from a teacher. However, both this and other reinforcement learning transfer methods have little theoretical analysis. This paper formally defines a setting where multiple teacher agents can provide advice to a student and introduces an algorithm to leverage both autonomous exploration and teacher's advice. Our regret bounds justify the intuition that good teachers help while bad teachers hurt. Using our formalization, we are also able to quantify, for the first time, when negative transfer can occur within such a reinforcement learning setting.},
archivePrefix = {arXiv},
arxivId = {1604.03986},
author = {Mnih, Volodymyr and Silver, David},
doi = {10.1038/nature14236},
eprint = {1604.03986},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {10450823},
journal = {Nature},
number = {7540},
pages = {2315--2321},
pmid = {25719670},
title = {{Human level control through Deep Reinforcement Learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {2016-Janua},
year = {2015}
}
@article{,
abstract = {Policy advice is a transfer learning method where a student agent is able to learn faster via advice from a teacher. However, both this and other reinforcement learning transfer methods have little theoretical analysis. This paper formally defines a setting where multiple teacher agents can provide advice to a student and introduces an algorithm to leverage both autonomous exploration and teacher's advice. Our regret bounds justify the intuition that good teachers help while bad teachers hurt. Using our formalization, we are also able to quantify, for the first time, when negative transfer can occur within such a reinforcement learning setting.},
archivePrefix = {arXiv},
arxivId = {1604.03986},
doi = {10.1038/nature14236},
eprint = {1604.03986},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2016 - Human-level control through deep reinforcement learning.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
number = {7540},
pages = {2315--2321},
pmid = {25719670},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {2016-Janua},
year = {2016}
}
@article{Sakama2009,
abstract = {Abstract  This paper introduces a novel logical framework for concept-learning called brave induction. Brave induction uses brave inference for induction and is useful for learning from incomplete information. Brave induction$\backslash$n  is weaker than explanatory induction which is normally used in inductive logic programming, and is stronger than learning from satisfiability, a general setting of concept-learning in clausal logic. We first investigate formal properties of brave induction, then$\backslash$n  develop an algorithm for computing hypotheses in full clausal theories. Next we extend the framework to induction in nonmonotonic logic programs. We analyze computational complexity of decision problems for induction on propositional theories. Further, we provide examples$\backslash$n  of problem solving by brave induction in systems biology, requirement engineering, and multiagent negotiation.},
author = {Sakama, Chiaki and Inoue, Katsumi},
doi = {10.1007/s10994-009-5113-y},
file = {:home/kiyo/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sakama, Inoue - 2009 - Brave induction A logical framework for learning from incomplete information.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Brave induction,Inductive logic programming,Nonmonotonic logic programming},
number = {1},
pages = {3--35},
title = {{Brave induction: A logical framework for learning from incomplete information}},
volume = {76},
year = {2009}
}
@article{Muggleton1991,
abstract = {A new research area, Inductive Logic Programming, is presently emerging. While inheriting various positive characteristics of the parent subjects of Logic Programming and Machine Learning, it is hoped that the new area will overcome many of the limitations of its forebears. The background to present developments within this area is discussed and various goals and aspirations for the increasing body of researchers are identified. Inductive Logic Programming needs to be based on sound principles from both Logic and Statistics. On the side of statistical justification of hypotheses we discuss the possible relationship between Algorithmic Complexity theory and Probably-Approximately-Correct (PAC) Learning. In terms of logic we provide a unifying framework for Muggleton and Buntine's Inverse Resolution (IR) and Plotkin's Relative Least General Generalisation (RLGG) by rederiving RLGG in terms of IR. This leads to a discussion of the feasibility of extending the RLGG framework to allow for the invention of new predicates, previously discussed only within the context of IR. {\textcopyright} 1991 Ohmsha, Ltd. and Springer.},
author = {Muggleton, S},
doi = {10.1007/BF03037089},
isbn = {0288-3635},
issn = {02883635 (ISSN)},
journal = {New Generation Computing},
keywords = {Learning,induction,information compression,inverse resolution,logic programming,predicate invention},
number = {4},
pages = {295--318},
title = {{Inductive logic programming}},
url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000640432{\&}doi=10.1007{\%}2FBF03037089{\&}partnerID=40{\&}md5=eca5117a0efba780a7b1b156fdc19036},
volume = {8},
year = {1991}
}
